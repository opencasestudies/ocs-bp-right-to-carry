---
title: "Open Case Studies: Examination of Multicollinearity Influence on Inference Using Right-to-Carry Gun Law and Violent Crime Data"
author: "Michael Ontiveros, Carrie Wright, PhD."
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>


---


```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE, fig.width=10, fig.height=7,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)

```




#### {.outline }
```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "mainplot.png"))
```

####

## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io){target="_blank"} project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

## {.license_block}

This work is licensed under the Creative Commons Attribution-NonCommercial 3.0 [(CC BY-NC 3.0)](https://creativecommons.org/licenses/by-nc/3.0/us/){target="_blank"}  United States License.

## {.reference_block}

To cite this case study please use:

Wright, Carrie, and Ontiveros, Michael and Jager, Leah and Taub, Margaret and Hicks, Stephanie. (2020). https://github.com/opencasestudies/ocs-right-to-carry-case-study/. Examination of Multicollinearity Influence on Inference Using Right-to-Carry Gun Law and Violent Crime Data (Version v1.0.0)

# **Motivation**
*** 

This case study will introduce the topic of multicollinearity. We will do so by showcasing a real world example where multicollinearity in part resulted in historically controversial and conflicting findings about the influence of the adoption of right-to-carry (RTC) concealed handgun laws on violent crime rates in the United States. 

We will focus on two articles:

1) The first analysis by [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"} published in 1996 suggests that RTC laws reduce violent crime. Lott authored a book extending these findings in 1998 called [***More Guns, Less Crime***](https://en.wikipedia.org/wiki/More_Guns,_Less_Crime){target="_blank"}.

```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "Lott.png"))
```

2) The second analysis is a recent article by [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} published in 2017 that suggests that RTC laws increase violent crime. Donohue has also published previous articles with titles such as [***"Shooting down the "More Guns, Less Crime" Hypothesis***](https://www.jstor.org/stable/1229603?seq=1){target="_blank"} 

```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "Donohue.png"))
```

This has been a controversial topic as many other articles also had conflicting results. See [here](https://en.wikipedia.org/wiki/More_Guns,_Less_Crime){target="_blank"} for a list of studies.

The [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} article discusses how there are many other important methodological aspects besides multicollinearity that could account for the historically conflicting results in these previous papers.

In fact, nearly every aspect of the data analysis process was different between the [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} analysis and the [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"} analysis.

```{r, echo=FALSE, out.height = '75%', out.width = '75%', fig.align='center'}
knitr::include_graphics(here("img", "Educational_Graphic1.jpg"))
```

However, we will focus particularly on multicollinearity and we will explore how it can influence linear regression analyses and result in different conclusions. 

This analysis will demonstrate how methodological details can be critically influential for our overall conclusions and can result in important policy related consequences. This [article]((https://www.nber.org/papers/w23510.pdf){target="_blank"}) will provide a basis for the motivation. 

#### {.reference_block}

John J. Donohue et al., Right‐to‐Carry Laws and Violent Crime: A Comprehensive Assessment Using Panel Data and a State‐Level Synthetic Control Analysis. *Journal of Empirical Legal Studies*, 16,2 (2019).

David B. Mustard & John Lott. Crime, Deterrence, and Right-to-Carry Concealed Handguns. *Coase-Sandor Institute for Law & Economics* Working Paper No. 41, (1996).

####


Here you can see the differences in the data used in the featured RTC articles:


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img",'Donohue_Table2.png'))
```


We will perform analyses similar to those in these articles, however **we will not try to recreate them**, instead we will simplify our analysis to allow us to focus on multicollinearity.


Therefore we will use a subset of the listed explanatory variables and they will be consistent for both analyses that we will perform, with the exception that one analysis will have 6 demographic variables like the analysis in the [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} article and the other will have 36 demographic variables like the analysis in the [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"} article.


# **Main Question**
*** 

#### {.main_question_block}
<b><u> Our main question: </u></b>

1) How does the inclusion of different numbers of age groups influence the results of an analysis of right to carry laws and violence rates?

####



# **Learning Objectives** 
*** 

<u>**Statistical Learning Objectives:**</u> 

1) what multicollinearity is and how it can influence linear regression coefficients  
2) how to look for the presence of multicollinarity and determine the severity
3) the difference between multicollinearity and correlation 
4) implementation of panel regression analysis (`plm`)
5) calculation of VIF (`car`)

<u>**Data science Learning Objectives:**</u>

1) data import of many different file types with special cases (`readr`, `readxl`, `pdftools`)
2) joining data from multiple sources (`dplyr`)  
3) working with character strings (`stringr`)
4) data comparisons (`dplyr` and `janitor`)
5) reshaping data into different formats (`tidyr`)  
6) visualizations (`ggplot2`) 
7) perform iterative simulations (`rsample`)


We will especially focus on using packages and functions from the [`Tidyverse`](https://www.tidyverse.org/){target="_blank"}, such as `dplyr` and `ggplot2`. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R especially efficient.


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

# **Context**
***

So what exactly is a **right-to-carry law**?

It is a law that specifies if and how citizens are allowed to have a firearm on their person or nearby (for example in the citizen's car) in public. 

The [Second Amendment](https://en.wikipedia.org/wiki/Second_Amendment_to_the_United_States_Constitution){target="_blank"} to the United States Constitution guarantees the right to "keep and bear arms". The amendment was ratified in 1791 as part of the [Bill of Rights](https://en.wikipedia.org/wiki/United_States_Bill_of_Rights){target="_blank"}.

```{r, echo=FALSE, out.height = '50%', out.width = '50%', fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/7/79/Bill_of_Rights_Pg1of1_AC.jpg")
```

However, there are no federal laws about carrying firearms in public. 

These laws are created and enforced at the state level. Sates vary greatly in their laws about the right to carry firearms. Some require extensive effort to obtain a permit to legally carry a firearm, while other states require very minimal effort to legally carry a firearm.


According to Wikipedia about the history of right-to-carry policies in the United States:

> Public perception on concealed carry vs open carry has largely flipped. In the early days of the United States, open carrying of firearms, long guns and revolvers was a common and well-accepted practice. Seeing guns carried openly was not considered to be any cause for alarm. Therefore, anyone who would carry a firearm but attempt to conceal it was considered to have something to hide, and presumed to be a criminal. For this reason, concealed carry was denounced as a detestable practice in the early days of the United States.

> Concealed weapons bans were passed in Kentucky and Louisiana in 1813. (In those days open carry of weapons for self-defense was considered acceptable; concealed carry was denounced as the practice of criminals.) By 1859, Indiana, Tennessee, Virginia, Alabama, and Ohio had followed suit. By the end of the nineteenth century, similar laws were passed in places such as Texas, Florida, and Oklahoma, which protected some gun rights in their state constitutions. Before the mid 1900s, most U.S. states had passed concealed carry laws rather than banning weapons completely. Until the late 1990s, many Southern states were either "No-Issue" or "Restrictive May-Issue". Since then, these states have largely enacted "Shall-Issue" licensing laws, with numerous states legalizing "Unrestricted concealed carry".

See [here](https://en.wikipedia.org/wiki/History_of_concealed_carry_in_the_U.S.){target="_blank"} for more information.

Here are the general categories of Right to Carry Laws:

```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "RTC.png"))
```
[source](https://www.nraila.org/gun-laws/){target="_blank"}


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "RTC_map.png"))
```

[source](https://www.nraila.org/gun-laws/){target="_blank"}

You can see that none of the fifty states have no-issue laws currently (the gray category), meaning that all states allow the right to carry firearms at least in some way, however the level of restrictions is dramatically different from one state to another.

Here you can see how these laws have changed over time around the country:
```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics("https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Right_to_Carry%2C_timeline.gif/620px-Right_to_Carry%2C_timeline.gif")
```

There is variation from state to state even within the same general category:

For example here are the [current carry laws in Idaho](https://www.nraila.org/gun-laws/state-gun-laws/idaho/) which is considered an "Unrestricted - no permit required" state:

>Idaho permits the open carrying of firearms.

>Idaho law permits both residents and non-residents who are at least 18 years old to carry concealed weapons, without a carry license, outside the limits of or confines of any city, provided the person is not otherwise disqualified from being issued a license to carry.

>A person may also carry concealed weapons on or about his or her person, without a license, in the person’s own place of abode or fixed place of business, on property in which the person has any ownership or leasehold interest, or on private property where the person has permission to carry from any person who has an ownership or leasehold interest in that property. 

>State law also allows any resident of Idaho or a current member of the armed forces of the United States to carry a concealed handgun without a license to carry, provided the person is over 18 years old and not disqualified from being issued a license to carry concealed weapons under state law. An amendment to state law that takes effect on July 1, 2020 changes the reference in the above law from “a resident of Idaho” to “any citizen of the United States.”  


And here are the [current carry laws in Arizona](https://www.nraila.org/gun-laws/state-gun-laws/arizona/) which is also considered an "Unrestricted - no permit required" state:

> Arizona respects the right of law abiding citizens to openly carry a handgun.

> Any person 21 years of age or older, who is not prohibited possessor, may carry a weapon openly or concealed without the need for a license. Any person carrying without a license must acknowledge and comply with the demands of a law enforcement officer when asked if he/she is carrying a concealed deadly weapon, if the officer has initiated an "investigation" such as a traffic stop.

Notice that citizens in Idaho only need to be 18 to carry a firearm, whereas they must be 21 in Arizona. 


In contrast here is an example of [current carry laws in Maryland](https://www.nraila.org/gun-laws/state-gun-laws/maryland/) which is considered a "Rights Restricted-Very Limited Issue" state:

> Carrying and Transportation in Vehicles
It is unlawful for any person without a permit to wear or carry a handgun, openly or concealed, upon or about his person.  It is also unlawful for any person to knowingly transport a handgun in any vehicle traveling on public roads, highways, waterways or airways, or upon roads or parking lots generally used by the public. This does not apply to any person wearing, carrying or transporting a handgun within the confines of real estate owned or leased by him, or on which he resides, or within the confines of a business establishment owned or leased by him.

> Permit To Carry
Application for a permit to carry a handgun is made to the Secretary of State Police. In addition to the printed application form, the applicant should submit a notarized letter stating the reasons why he is applying for a permit.


# **Limitations**
*** 
There are some important considerations regarding this data analysis to keep in mind: 

1) We do not use all of the data used by either the [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"} or [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} analyses, nor do we perform the same analysis of each article. We instead perform a much simpler analysis with less variables for the purposes of illustration of the concept of multicollinearity and its influence on regression coefficients, not to reproduce either analysis.

2) Our analysis accounts for either the adoption or lack of adoption of a permissive right-to-carry law in each state, but does not account for differences in the level of permissiveness of the laws.

Recall that these are the categories of right to carry laws:
```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "RTC.png"))
```
```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "RTC_map.png"))
```
States with laws of the category rights restricted - very limited issue (red) are considered as not having a permissive right-to-carry law. Recall that no states currently have a rights infringed/non-issue law.

States of all other categories (shall issue, discretionary/reasonable issue, and no permit required) (all shades of blue) are considered the same in our analysis, as having a permissive right-to-carry law.

3) Because our analysis is an oversimplification, our analysis should not be used for determining policy changes, instead we suggest that users consult with a specialist.

avocado - this is from Michael and very important... not sure how we want to word this...

It is important to note that we do not treat race as an objective measure. Despite this, it can be used to advance scientific inquiry. For more information on this topic, we have included a link to a [paper on the use of race as a measure in epidemiology](https://academic.oup.com/epirev/article/22/2/187/456942). 


We will begin by loading the packages that we will need:

```{r}
library(here)
library(readxl)
library(readr)
library(pdftools)
library(dplyr)
library(magrittr)
library(tidyr)
library(stringr)
library(purrr)
library(forcats)
library(tibble)
library(car) # vif function
library(plm) # fixed effect model, linear regression
library(broom) # tidy output
library(cowplot) # to produce plot of plots 
library(GGally)
library(ggrepel)
library(scales)
library(latex2exp)
library(viridis)
library(ggcorrplot)
library(rsample)

set.seed(999)
```


 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data
[car]  | to calculate VIF values
[purrr] | to combine multiple tibbles within a list of tibbles
[forcats] | to collapse levels of factors into more summarized versions
The first time we use a function, we will use the `::` to indicate which package we are using. Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.


# **What are the data?**
***

Below is a table from the [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} paper that shows the data used in both analyses, where DAW stands for [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} and LM stands for [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"}.


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "Donohue_AppendixJ.png"))
```

We will be using a subset of these variables, which are highlighted in green:


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img", "ourdata.png"))
```


# **Data Import**
***



## Demographic and population data

To obtain information about age, sex, and race, and overall population we will use US Census Bureau data, just like both of the articles. The census data is available for different time spans. Here are the links for the years used in our analysis. We will use data from 1977 to 2010.

Data   | Link                                                                        
---------- |-------------
**years 1977 to 1979**  | [link](https://www2.census.gov/programs-surveys/popest/tables/1900-1980/state/asrh/)  
**years 1980 to 1989**  | [link](https://www2.census.gov/programs-surveys/popest/tables/1980-1990/counties/asrh/) * county data was used for this decade which also has state information
**years 1990 to 1999**  | [link](https://www2.census.gov/programs-surveys/popest/tables/1990-2000/state/asrh/)
**years 2000 to 2010**  | [link](https://www.census.gov/data/datasets/time-series/demo/popest/intercensal-2000-2010-state.html) <br> [technical documentation](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2000-2010/intercensal/state/st-est00int-alldata.pdf){target="_blank"}

To import the data we will use the `read_csv()` function of the `readr` package for the csv files. In some decades, there are separate files for each year, we will read each of these together using the base `list.files()` function to get all of the names for each file and then the `map()` function of the `purrr` package to apply the `read_csv()` function on all of the file paths in the list created by `list.files()`. For years that are txt files we will use `read_table2()` also for the `readr` package. The `read_table2()` function, unlike the `read_table()`,  allows for any number of white space characters between columns, and the lines can be of different lengths.

AVOCADO I am a bit confused about the last decade... it's only one file but it seems to need map...

```{r}

dem_77_79 <- read_csv("docs/Demographics/Decade_1970/pe-19.csv", skip = 5)

dem_80_89 <- list.files(recursive = TRUE,
                  path = "docs/Demographics/Decade_1980/",
                  pattern = "*.csv",
                  full.names = TRUE) %>% 
  map(~read_csv(., skip=5))

dem_90_99 <- list.files(recursive = TRUE,
                  path = "docs/Demographics/Decade_1990/",
                  pattern = "*.txt",
                  full.names = TRUE) %>% 
  map(~read_table2(., skip = 14))


dem_00_10_2 <- read_csv("docs/Demographics/Decade_2000/st-est00int-alldata.csv")

dem_00_10 <- list.files(recursive = TRUE,
                  path = "docs/Demographics/Decade_2000/",
                  pattern = "*.csv",
                   full.names = TRUE) %>% 
   map(~read_csv(.))

head(dem_00_10)

```

Notice that the `STATE` variable for the demographic data is numeric. That is because it is encoded by [Federal Information Processing Standard (FIPS) state codes](https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code){target="_blank". Thus we also need to import data  about FIPS encoding so that we can identify what data corresponds to what state.


## State FIPS codes



The following data was downloaded from the [US Census Bureau](https://www.census.gov/geographies/reference-files/2014/demo/popest/2014-geocodes-state.html){target="_blank"}.

To import the data we will use the `read_xls()` function of the `readxl` package. Since the first five lines of this excel is information about the source of the data and when it was released, we need to skip importing these lines using the `skip` argument so that the data has the same number of columns for each row. 

```{r, out.width = "500 px"}
knitr::include_graphics(here("img", "FIPS.png"))

```

```{r}
STATE_FIPS <- read_xls("docs/State_FIPS_codes/state-geocodes-v2014.xls", skip = 5)
(STATE_FIPS)
```

## Police staffing data
The following data was downloaded from the [Federal Bureau of Investigation](https://crime-data-explorer.fr.cloud.gov/downloads-and-docs). 


The `read_csv()` function of the `readr` package guesses what the class is for each variable, but sometimes it makes mistakes. It is good to specify the class for variables if you know them. We know that we want the variables about male and female counts to be numeric. We can specify that using the `col_types =` argument. See [here](https://readr.tidyverse.org/articles/readr.html) and [here](https://cran.r-project.org/web/packages/readr/vignettes/readr.html) for more information.

```{r}
ps_data <- read_csv("docs/Police_staffing/pe_1960_2018.csv")
ps_data <- read_csv("docs/Police_staffing/pe_1960_2018.csv",
                    col_types = cols(male_total_ct = "n",
                                     female_total_ct = "n"))

ps_data <- read_csv("docs/Police_staffing/pe_1960_2018.csv",
                   col_types =  cols(male_total_ct = col_double(),
                                   female_total_ct = col_double()))
head(ps_data)                            
```



## Unemployment data

The following data was downloaded from the [U.S. Bureau of Labor Statistics](https://data.bls.gov/cgi-bin/dsrv?la). 

There are excel files for each state.  As you can see, there are many rows to skip to make sure that there are the same number of columns for each row. We can also see that the state name is located in a couple of the first rows. 

```{r}
knitr::include_graphics(here("img", "Unemp.png"))
```

We can also see that here if we just try to read in the files directly.

```{r}

ue_rate_data <- list.files(recursive = TRUE,
                  path = "docs/Unemployment",
                  pattern = "*.xlsx",
                  full.names = TRUE) %>% 
  map(~read_xlsx(.))
      
head(ue_rate_data)[1]
```

So now we will skip the first 10 lines. And also create a names tibble that contains only the cell with the state information.

```{r}
 
 ue_rate_data <- list.files(recursive = TRUE,
                  path = "docs/Unemployment",
                  pattern = "*.xlsx",
                  full.names = TRUE) %>% 
  map(~read_xlsx(., skip = 10))
  
head(ue_rate_data[1])
```

To get the state name for each file using the `map()` function to perform functions across all of the files, we will specifically import only a small range of cells using the `range = ` argument and then grab the cell that has state information based on it's location within the range of cells imported using `c()` and then use the base `unlist()` function to unlist the list that this creates.

```{r}
ue_rate_names <- list.files(recursive = TRUE,
                  path = "docs/Unemployment",
                  pattern = "*.xlsx",
                  full.names = TRUE) %>%
  map(~read_xlsx(., range = "B4:B6")) %>%
  map(., c(1,2)) %>%
  unlist()

ue_rate_names
```

Now we will make these values the names of the different tibbles within `ue_rate_data`.
```{r}
names(ue_rate_data) <- ue_rate_names
```

## Poverty data
Extracted from Table 21 from [US Census Bureau Poverty Data ](https://www.census.gov/data/tables/time-series/demo/income-poverty/historical-poverty-people.html)

AVOCado strange issue that both Michael and I had

```{r}

#**persistent warning from unknown origin** https://community.rstudio.com/t/persistent-unknown-or-uninitialised-column-warnings/64879

#solution to above is alledgedly: "In any case the suggested approach is to initialize the column"


poverty_rate_data <- read_xls("docs/Poverty/hstpov21.xls", skip=2) #This may cause initialization issue, not easily reproducible (even after restarting R)

head(poverty_rate_data)
```

We can see that this will require some wrangling to make the data more usable. 

## Violent crime

Violent crime data was obtained from [here](https://www.ucrdatatool.gov/Search/Crime/State/StatebyState.cfm) This data is a bit trickier because of spaces and `/` in the column names, thus the `read_lines()` function of the `readr` package works better than the `read_csv()` function.


```{r}
knitr::include_graphics(here("img", "crime.png"))
```

```{r}
crime_data <- read_lines("docs/Crime/CrimeStatebyState.csv", skip = 2, skip_empty_rows = TRUE)
head(crime_data)

```

We can see that this data will also require some wrangling to make it more usable. 

## Right-to-carry data

This data is extracted from table in [Donohue paper](https://www.nber.org/papers/w23510.pdf) {target="_blank"}. We will use the function `pdf_text()`  of the `pdftools` package to import the pdf document.

```{r}

if(!file.exists(here("docs", "w23510.pdf"))){
  url <- "https://www.nber.org/papers/w23510.pdf"
  utils::download.file(url, here("docs", "w23510.pdf"))
}

DAWpaper <- pdf_text(here("docs", "w23510.pdf"))

head(DAWpaper[1])

```

Again, this data will also require quite a bit of wrangling.



# **Data Wrangling**
***
## State FIPS codes

Let's first take a look at our state FIPS data to see if it needs any cleaning or reshaping. We should start with this data, because we will need to use it to wrangle some of the other data.

```{r}
head(STATE_FIPS)
```

We only need the last two columns, but we might want to rename them. The `Name` variable is vague. The variable with the FIPS code is called `State\n(FIPS)`. To get rid of the new line in this variable name and to change the `Name` variable to something more informative, we will use the `rename()` function of the `dplyr` package.  To use this function, we need to list the new name first followed by `=` and then the existing variable. We can rename multiple variables at the same time by using a comma to separate the variables we are renaming. We will use the `select()` function also of the `dplyr` package just to keep these variables, and we will filter out the rows with FIPS values of `00` with the `filter()` function, again also part of the `dplyr` package. we will specify that we want `STATEFP` values that are not equal to `00` by using this operator: `!=`. We will also use the double pipe operator `%<>%` of the `magrittr` package which allows us to use data as input and then reassign it after we perform sum functions using it.

```{r}

STATE_FIPS %<>% 
dplyr::rename( STATEFP = `State\n(FIPS)`,
                 STATE = Name) %>%
    dplyr::select(STATEFP, STATE) %>%
    dplyr::filter(STATEFP != "00")

STATE_FIPS

```

## Demographic and population data


<details><summary> Click here to see detailed information about how the demographic data was wrangled </summary>


<font size="6"> **1977-1979**</font>

***


Now let's take a look at our demographic data across the decades that we wish to study. If you have very wide data (meaning it has many columns), one way to view the data so that you can see all of the columns at the same time is to use the `glimpse()` function of the `dplyr` package. 

Taking a look at the first decade of data, we can see that the `Race/Sex Indicator` contains two types of data, the race and the sex. This does not follow the tidy data philosophy, where each cell of a tibble should only contain one piece of information. Typically one might think of using the `separate()` function of the `tidyr` package to split this variable into two. However, one of the race values is `Other races` and since this also has a space, this makes separating this data more tricky.

Instead we will use the `str_extract()` function of the `stringr` package and the `mutate()` function of the `dplyr` package. The "mutate()" will allow us to create new variables, and "str_extract()" function  will allow us to match specific patterns and pull out matches to those patterns. Therefore, if the `Race/Sex Indicator` value is `Other races male` and if we extract patterns matching either `"male"` or `"female"` which we can specify like this `pattern = "male|female"` then, the value will be `male`.

First we need to rename the `Race/Sex Indicator` variable to not have spaces so that it is compatible with the `str_extract()` function.

We also want to rename a couple of variables to be simpler and filter the data to only include the years of the data we are interested in, as well as remove some variables that we don't need like the `FIPS State Code`. We can remove variables by using the `select()` function with a `-` minus sign in front of the variable we wish to remove.

```{r}
dplyr::glimpse(dem_77_79)


dem_77_79 <- dem_77_79 %>%
  rename("race_sex" =`Race/Sex Indicator`) %>%
  mutate(SEX = str_extract(race_sex, "male|female"),
        RACE = str_extract(race_sex, "Black|White|Other"))%>%
  select(-`FIPS State Code`, -`race_sex`) %>%
  rename("YEAR" = `Year of Estimate`,
        "STATE" = `State Name`) %>%
  filter(YEAR %in% 1977:1979)

glimpse(dem_77_79)
```

That's looking pretty  good! We also want to take all the age group variables and make one variable that is the age group name and one that is the value of the population count for that age group. To do this we will use the `pivot_longer()` function of the `tidyr` package. To use this function, we need to use the `cols` argument to indicate which columns we want to pivot. We also name the new variables we will create with the `names_to` and `values_to` arguments. The `names_to` will be the name of the variable that will identify each age group and `values_to` will be the name of the variable that contains the corresponding population values.
```{r}
dem_77_79 <- dem_77_79 %>%
  pivot_longer(cols=contains("years"),
               names_to = "AGE_GROUP",
               values_to = "SUB_POP")

glimpse(dem_77_79)
```

We also want to get data about the total population for the state for each year.

To do so we can sum all the values for the `SUB_POP` variable that we just created. To do this we can use the `group_by` and `summarise()` functions of the `dplyr` package. The `group_by()` function specifies how we want to calculate  our sum, that we would like to calculate it for each year and each state individually. Thus, all the values that have the same `STATE` and `YEAR` values will be summed together, rather than summing using all of the values in the `SUB_POP` variable. The `.groups` argument allows us to remove the grouping after we perform the calculation with `summarise()`.

```{r}
pop_77_79 <- dem_77_79 %>%
  group_by(YEAR, STATE) %>%
  summarise("TOT_POP" = sum(SUB_POP), .groups = "drop") 

pop_77_79 
```


 Now we will add the population value to the demographic tibble using the `left_join()` function of the `dplyr` package. It is important that we specify how this should be done, that the `YEAR` and `STATE` variable values should match each other. This will place the `dem_77_79` variables to the left of the `pop_77_79` data. 
 
```{r}
dem_77_79 <- dem_77_79 %>%
  left_join(pop_77_79, by = c("YEAR","STATE"))

dem_77_79
```

We will also calculate the percentage that each group makes up of the total population, by dividing the `SUB_POP` by the `TOT_POP` and multiplying by 100 using the `mutate()` function. we will also remove the other population variables.

```{r}
dem_77_79 %<>%
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) %>%
  select(-SUB_POP, -TOT_POP)

dem_77_79
```
It is important to make sure that we have the total values we would expect. We have two levels of `SEX`, three levels of `Race`, three levels of `YEAR`, eighteen levels of `AGE_GROUP`, and fifty one levels of `STATE`. If we multiply this together we get 16,524 which is the same as the number of rows in our final `dem_77_79` data. Looks good!

Also Let's make the values of the `SEX` variable capitalized so that they match the other values of the other variables like `RACE` etc. This will help us to keep consistent values across the different years as we wrangle the data for the other decades. To do so we will use the `str_to_title()` function of the `stringr` package. We need to use the `pull()` function to get the values of `SEX` out of `dem_77_79`. Once we make them capitalized they are then reassigned to the `SEX` variable. 

```{r}

dem_77_79 %<>%
  mutate(SEX = str_to_title(pull(dem_77_79, SEX)))

# This can also be done line this:
dem_77_79 %<>%
  mutate(SEX = str_to_title(pull(., SEX)))
```

<font size="6"> **1980-1989**</font>

***


For this decade each year is a separate tibble and they are combined as a list.
```{r}
class(dem_80_89)
```

So the first thing we need to do is combine each tibble of the list together. We can do that using the `bind_rows()` function of `dplyr` which appends the data together based on the presence of columns with the same name in the different tibbles. We will use the `map_df()` function of the `purrr` package to allow us to do this across each tibble in our list. 

```{r}
dem_80_89 <- dem_80_89 %>%
  map_df(bind_rows)

glimpse(dem_80_89 )
```

Great! Now our data is all together.

Now we will wrangle the data similarly to the previous decade.
```{r}
dem_80_89 <- dem_80_89 %>%
  rename("race_sex" =`Race/Sex Indicator`) %>%
  mutate(SEX = str_extract(race_sex, "male|female"),
        RACE = str_extract(race_sex, "Black|White|Other"))%>%
  select( -`race_sex`) %>%
  rename("YEAR" = `Year of Estimate`)
         
glimpse(dem_80_89)
```
Notice that this time the state information is based on the numeric FIPS value. We want only the first two values, as the rest indicate the county. We can use the `str_sub()` function of the `stringr` package for this. We will specify that we want to start at the first position and end at the second.  Just like `str_extract()` we need to rename this variable first so that it is compatible. 
```{r}
dem_80_89 %<>%
rename("STATEFP_temp" = "FIPS State and County Codes") %>%
mutate(STATEFP = str_sub(STATEFP_temp, start = 1, end = 2)) %>%
    left_join(STATE_FIPS, by = "STATEFP") %>%
  dplyr::select(-STATEFP)

glimpse(dem_80_89)
```


```{r}
dem_80_89 %<>%
  pivot_longer(cols=contains("years"),
               names_to = "AGE_GROUP",
               values_to = "SUB_POP_temp") %>%
  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) %>%
  summarise(SUB_POP = sum(SUB_POP_temp), .groups="drop")

dem_80_89
```
  
```{r}
pop_80_89 <- dem_80_89 %>%
  group_by(YEAR, STATE) %>%
  summarise("TOT_POP" = sum(SUB_POP), .groups = "drop") 


dem_80_89 <- dem_80_89 %>%
  left_join(pop_80_89, by = c("YEAR","STATE")) %>%
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) %>%
  dplyr::select(-SUB_POP, -TOT_POP)

dem_80_89
```

Just like with the data from the 70s we will also change the values for `SEX` to be capitalized.

```{r}
dem_80_89 %<>%
  mutate(SEX = str_to_title(pull(., SEX)))
```

Again, it is important to make sure that we have the total values we would expect. This time we have: two levels of `SEX`, three levels of `Race`, ten levels of `YEAR`, eighteen levels of `AGE_GROUP`, and fifty one levels of `STATE`.

If we multiply these together we get 55,080, which is the same as the number of rows of the final `dem_80_89` data. Looks good!

<font size="6"> **1990-1999**</font>

***

Just like the 80s we need to combine the data across the files:

```{r}
dem_90_99 <- dem_90_99 %>%
  map_df(bind_rows)
```

```{r}
glimpse(dem_90_99)
```
For this decade the column names can't all be imported in a simple way from the table, so they need to be recoded.

Here is what the data looks like before importing:

```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "90.png"))
```

So, first using the base `colnames()` function we change the names of the column names.

```{r}

colnames(dem_90_99) <- c("YEAR",
                         "STATEFP",
                         "Age",
                         "NH_W_M",
                         "NH_W_F",
                         "NH_B_M",
                         "NH_B_F",
                         "NH_AIAN_M",
                         "NH_AIAN_F",
                         "NH_API_M",
                         "NH_API_F",
                         "H_W_M",
                         "H_W_F",
                         "H_B_M",
                         "H_B_F",
                         "H_AIAN_M",
                         "H_AIAN_F",
                         "H_API_M",
                         "H_API_F")

glimpse(dem_90_99)
```

Notice also that the first row is all `NA` values from white space in the original table for 1990, this is probably true for each year. We can check them dimensions of our table using the base `dim()` function. When we filter for rows where `YEAR` is `NA`, we indeed see 10 rows, which is what we would expect if we have a row like this for each of the years in the decade. We see the same if we try a different variable. Now we will test to see how large our tibble is if we drop rows with `NA` values using the `drop_na()` function of `tidyr`. We that indeed our dimensions only changed by ten, so there are not other rows with missing values that we might not expect. So now we will resign the `dem_90_99` variable after removing these rows.

```{r}

dim(dem_90_99)

dem_90_99 %>%
  filter(is.na(YEAR))

dem_90_99 %>%
  filter(is.na(Age)) 

dem_90_99 %>%drop_na() 

dem_90_99 %<>%drop_na() 
```

Then we sum across the non-hispanic and Hispanic groups because this information is not available for the other previous decades. Then we will remove the variables for the Hispanic and non-Hispanic subgroups using `select()`.

```{r}

dem_90_99%<>%
    mutate(W_M = NH_W_M + H_W_M,
           W_F = NH_W_F + H_W_F,
           B_M = NH_B_M + H_B_M,
           B_F = NH_B_F + H_B_F,
           AIAN_M = NH_AIAN_M + H_AIAN_M,
           AIAN_F = NH_AIAN_F + H_AIAN_F,
           API_M = NH_API_M + H_API_M,
           API_F = NH_API_F + H_API_F) %>%
  select(-starts_with("NH_"), -starts_with("H_"))

glimpse(dem_90_99)
```

Looking better! We also need to add age groups like the other decades. We will take a look at the 80s data using the `distinct()` function of the `dplyr` package to see what age groups we need. We can use the base `cut()` function to create a new variable with `mutate()` called `AGE_GROUP` that will have a label for every change in 5 years of age. The `right = FALSE` argument specifies that the interval is not closed on the right, meaning that if the value is at the cut point like the `Age` value is 5, then it will be in the `5 to 9 years` group.

We can make the labels for the `AGE_GROUP` variable match those of `dem_77_79` but we need to pull out the values of the tibble created by `distinct()`. To do this we can use the `pull()` function from the `dplyr` package. Note that it is important to check that the `AGE_GROUP` values are listed in order for `dem_77_79`. We will also remove the `Age` variable after we create the new `AGE_GROUP` variable for the `dem_90_99` data. 


```{r}

distinct(dem_77_79, AGE_GROUP)
pull(distinct(dem_77_79, AGE_GROUP))

dem_90_99 %<>%
  mutate(AGE_GROUP = cut(Age,
                         breaks = seq(0,90, by=5),
                         right = FALSE, labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP))) %>%
  select(-Age)

glimpse(dem_90_99)

```

Like the previous decades we will create a `RACE` and `SUB_POP` variable using `pivot_longer()` to create a single `Race` variable out of all the subgroup variables. 

Now we need to collapse the data for the various races so that it matches the previous decades. This time we will use the `case_when()` function of the `dplyr` package and the `str_detect()` function of the `stringr` package to identify when the race is something other than `B` or `W` and replace with the value `Other`. The value to the right of the `~` indicates what we want the value of the new variable to be if the value of the variable we are using with `str_decect()` matches the condition specified. If the value does not match the specified condition, than the other values will be what ever is listed after `TRUE ~`. We will then create population counts as we did previously for the other decades.

Finally, we will create new sums for the sub-populations where we sum across the two `Other` subgroups `Race`  to a create a single value for each value of `YEAR`, `SEX`, `AGE_GROUP`, and `STATE` by using the `group_by()` function and `summarie()`.  

```{r}
dem_90_99  %<>%
  pivot_longer(cols = c(starts_with("W_"),
                    starts_with("B_"),
                    starts_with("AIAN_"),
                    starts_with("API_")),
               names_to = "RACE",
               values_to = "SUB_POP_temp")

dem_90_99 %<>%
  mutate(SEX = case_when(str_detect(RACE, "_M") ~ "Male",
                         TRUE ~ "Female"),
         RACE = case_when(str_detect(RACE, "W_") ~ "White",
                          str_detect(RACE, "B_") ~ "Black",
                          TRUE ~ "Other")) %>%
  left_join(STATE_FIPS, by = "STATEFP") %>%
  dplyr::select(-STATEFP)

dem_90_99 %<>%
  group_by(YEAR, STATE, AGE_GROUP, SEX, RACE) %>%
  summarise(SUB_POP = sum(SUB_POP_temp), .groups="drop")

```

```{r}
pop_90_99 <- dem_90_99 %>%
  group_by(YEAR, STATE) %>%
  summarise(TOT_POP = sum(SUB_POP), .groups = "drop")

dem_90_99 <- dem_90_99 %>%
  left_join(pop_90_99, by=c("YEAR", "STATE")) %>%
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) %>%
  dplyr::select(-SUB_POP, -TOT_POP)

dem_90_99
```


Again, we should check to make sure that we have the total values we would expect. We have the same number of unique values for each of our variables as in with the data from the 80s, so if we collapsed the data for the different additional sub-populations in this data, then we have done it correctly. 

Indeed it looks like we have 55,080 rows, which is what we would expect and is the same as the number of rows of the final `dem_80_89` data. Looks good!

<font size="6"> **2000-2010**</font>

***

Again, for this decade we need to combine the data across years.

```{r}
dem_00_10 <- dem_00_10 %>%
  map_df(bind_rows)

glimpse(dem_00_10)

```

OK, the data looks a bit different from the others. First we will remove a couple of variables that we probably don't need. Also it looks like we have some values for the entire United Sates and we will drop these to be like the other decades.



```{r}
dem_00_10 %<>%
  select(-ESTIMATESBASE2000,-CENSUS2010POP) %>%
  filter(NAME != "United States")
```

We can see that there are lots of values that are zero. According to the [technical documentation](https://www2.census.gov/programs-surveys/popest/technical-documentation/file-layouts/2000-2010/intercensal/state/st-est00int-alldata.pdf){target="_blank"} for this data, zero values indicate the total for the other categories of `Sex`, `Origin`, `Race`, and `AGEGRP`.


```{r, echo = FALSE, out.width = "600 px"}
knitr::include_graphics(here::here("img", "tech_info.png"))
```

So we will drop the total values for `SEX`, `RACE`, and `AGEGRP` by removing the rows where these variables are equal to zero.

We will also want to only select for the total values for `Origin` as we do not wish to divide the data into subgroups about Hispanic ethnicity because we do not have that information for the first two decades. Thus we will filter for only the rows where `Origin` is equal to zero.

We will also then remove the `REGION`, `Division`, `STATE`, and `Origin` variables. We will then rename `NAME` to be `STATE` and rename `AGEGRP` to be like the other decades as `AGE_GROUP`.

```{r}
dem_00_10 %<>%
  filter(SEX != 0,
         RACE != 0,
         AGEGRP != 0, 
         ORIGIN == 0) %>%
  dplyr::select(-REGION, -DIVISION, -ORIGIN, -STATE) %>%
  rename("STATE" = NAME,
         "AGE_GROUP" = AGEGRP)

dem_00_10
```


Now we need to recode the numeric values to the values in the technical documentation. We can do so by adding labels to each numeric level using the base function `factor()`.

```{r}
dem_00_10 %<>%
  mutate(SEX = factor(SEX,
                            levels = 1:2,
                            labels = c("Male",
                                    "Female")),
         RACE = factor(RACE,
                            levels = 1:6,
                            labels = c("White",
                                    "Black",
                                    rep("Other",4))),
         AGE_GROUP = factor(AGE_GROUP,
                            levels = 1:18,
                            labels = pull(distinct(dem_77_79,AGE_GROUP), AGE_GROUP)))
                            
glimpse(dem_00_10)
```

OK, we also want to change the shape of the data so that we have a `YEAR` variable and each estimate of the population is a value in a new variable called `SUB_POP_temp`. 

```{r}
dem_00_10 %<>%
  pivot_longer(cols=contains("ESTIMATE"),
               names_to = "YEAR",
               values_to = "SUB_POP_temp")
```

We will now clean up the `YEAR` variable to only be the numeric value by keeping only the last 4 values of each string using the `str_sub()` function of the `stringr` package.

```{r}
dem_00_10 %<>%
  mutate(YEAR = str_sub(YEAR, start=-4)) %>%
  mutate(YEAR = as.numeric(YEAR))
```


Now we will collapse the data for the different RACES and calculate a new `SUB_POP` value. 

```{r}
dem_00_10 %<>%
  group_by(YEAR, AGE_GROUP, STATE, SEX, RACE) %>%
  summarise(SUB_POP = sum(SUB_POP_temp), .groups = "drop")
```

Again, the dimensions look as we expect with 60,588 rows. This time we have two levels of `SEX`, three levels of `Race`, **11** levels of `YEAR`, eighteen levels of `AGE_GROUP`, and fifty one levels of `STATE`. If we multiply this together we get 16,588. Looks good!

Now we will calculate the total population and percent of the total as we have done with the previous decades.


```{r}
pop_00_10 <- dem_00_10 %>%
  group_by(YEAR, STATE) %>%
  summarise(TOT_POP = sum(SUB_POP), .groups = "drop")
```

We can also check that our wrangling was performed correctly by summing the values for the individual sub-populations percentages and seeing if it totals to 100.

```{r}
dem_00_10 %>%
  left_join(pop_00_10, by=c("YEAR", "STATE")) %>%
  group_by(YEAR, STATE) %>%
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) %>%
  summarise(perc_tot = sum(PERC_SUB_POP), .groups = "drop") %>%
  mutate(poss_error = case_when(abs(perc_tot - 100) > 0 ~ TRUE,
                                TRUE ~ FALSE)) %>%
  group_by(poss_error) %>%
  tally()

```

Looks like the percentages for each state for each year all add up to 100, as we would expect. Great! Now we will reassign the `dem_00_10` data with this processing. 

```{r}
dem_00_10 %<>%
  left_join(pop_00_10, by = c("YEAR", "STATE")) %>%
  mutate(PERC_SUB_POP = (SUB_POP/TOT_POP)*100) %>%
 select(-SUB_POP, -TOT_POP)

dem_00_10
```

OK, now we are ready to combine all of our demographic data together!

<!-- </details> -->

## Combining demographic data

We can check that the column names are the same for the data for each of the decades by using the `setequal()` function of the `dplyr` package.

```{r}
setequal(colnames(dem_77_79),colnames(dem_80_89))
setequal(colnames(dem_80_89),colnames(dem_90_99))
setequal(colnames(dem_90_99),colnames(dem_00_10))
```


We can also confirm that we have the same number of age groups for each decade by using the base `length()` function. If you did not take a look at the wrangling for the demographic data then you may be unfamiliar with the `pull()` function of the `dplyr` package. This allows you to grab the values of a variable from a tibble. The `distinct()` function which is also of the `dplyr` package creates a tibble of the unique values for a variable.

```{r}
length(pull(distinct(dem_77_79, AGE_GROUP), AGE_GROUP))
length(pull(distinct(dem_80_89, AGE_GROUP), AGE_GROUP))
length(pull(distinct(dem_90_99, AGE_GROUP), AGE_GROUP))
length(pull(distinct(dem_00_10, AGE_GROUP), AGE_GROUP))
```

Looks good!


Now we will combine the data using the `bind_rows()` function of the `dplyr` package. This function appends the data together based on the presence of columns with the same name in the different tibbles.

```{r}
dem <- bind_rows(dem_77_79,
                 dem_80_89,
                 dem_90_99,
                 dem_00_10)
```


```{r}
glimpse(dem)
```

Great! now we have a really large single tibble.

Now we want to select similar demographic data to what was used in the previous analyses.

Here is the table from the [Donohue paper](https://www.nber.org/papers/w23510.pdf){target="_blank"} that compares the data used in the analyses.


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img",'Donohue_Table2.png'))
```
We can see that only the percentage of males that were from age 15-39 of the race groups (black, white, and other) were used in the Donohue analysis.

Ultimately we intend to make a tibble of data that is similar to each analysis. Therefore, we will create a data tibble about the demographic data for each analysis now.

To do so we will first create a vector of the age groups that should be included in the Donohue-like analysis, that we will call `DONOHUE_AGE_GROUPS`. We will then filter for only the age groups in this vector by using the `filter()` function of the `dplyr` package and the `%in%` operator to indicate that we want to keep all `AGE_GROUP` values that are equal to those within `DONOHUE_AGE_GROUPS`. We also want to filter for only population percentages for males by using the `==` operator. Then we can collapse the age groups from 20-39 by using the `fct_collpase()` function of the `forcats` package.

```{r}
DONOHUE_AGE_GROUPS <- c("15 to 19 years",
                        "20 to 24 years",
                        "25 to 29 years",
                        "30 to 34 years",
                        "35 to 39 years")

dem_DONOHUE <- dem %>%
  filter(AGE_GROUP %in% DONOHUE_AGE_GROUPS,
               SEX == "Male") %>%
  mutate(AGE_GROUP = fct_collapse(AGE_GROUP, "20 to 39 years"=c("20 to 24 years",
                                                                "25 to 29 years",
                                                                "30 to 34 years",
                                                                "35 to 39 years")))

dem_DONOHUE
```

We also want to create a new variable that will contain all the demographic information for each percentage just as was done in the [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} analysis. This should result in 6 different demographic variables.

To do this we will modify the `AGE_GROUP` variable by using the `mutate()` function of the `dplyr` package. We will replace the spaces in the now two age group categories with and underscore using the `str_replace_all()` function of the `stringr` package which replaces all instances of a pattern in a character string. 

Then we will use the `group_by()` function and the `summarise()` function also of the `dplyr` package to allow us to calculate a sum of the percentages for each of the sub-population percentages for the newly modified age groups in `AGE_GROUP`. The `.groups = "drop"` argument allows for the grouping to be removed after the `summarise()` function.

```{r}
dem_DONOHUE %<>%
  mutate(AGE_GROUP = str_replace_all(string = AGE_GROUP, 
                                     pattern = " ", 
                                     replacement = "_")) %>%
  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) %>%
  summarise(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = "drop")

dem_DONOHUE
```

Now we will combine the variables `RACE`, `SEX`, and `AGE_GROUP` together into one string separated by underscores using the `unite` function of the `tidyr` package. we will call this new variable `VARIABLE`.
We will rename the `PERC_SUB_POP` variable to be `VALUE` using the `rename()` function of the `dplyr` package. The new name should be listed first before the `=`.

```{r}
dem_DONOHUE %<>%
  unite(col = "VARIABLE", RACE, SEX, AGE_GROUP, sep = "_") %>%
  rename("VALUE" = PERC_SUB_POP)

dem_DONOHUE
```

Let's do a quick row number check. We have six different demographic variables, 51 states (DC counts as a state in this case), and 34 different years from 1977 to 2010, we should have 10,404 rows, which we do!

Now, let's do the same for the "Lott-like" analysis.


```{r, echo=FALSE, out.height = '100%', out.width = '100%', fig.align='center'}
knitr::include_graphics(here("img",'Donohue_Table2.png'))
```

So, in this analysis there were 36 variables covering percentages of individuals from 10 to over 65, three  race groups and both males and females. This table is misprinted and does not include the word "Other" for the third race group that was used. 

First we will filter out the age groups that were not included. Then we will collapse the age groups to those that were used by Lott et al. again using the `fct_collpase()` function of the `forcats` package. 

Also we will again combine the values across the variables to create a new demographic variable with 36 levels. 

```{r}
LOTT_AGE_GROUPS_NULL <- c("Under 5 years",
                          "5 to 9 years")

dem_LOTT <- dem %>%
  filter(!(AGE_GROUP %in% LOTT_AGE_GROUPS_NULL) )%>%
  mutate(AGE_GROUP = fct_collapse(AGE_GROUP,
                                  "10 to 19 years"=c("10 to 14 years",
                                                     "15 to 19 years"),
                                  "20 to 29 years"=c("20 to 24 years",
                                                     "25 to 29 years"),
                                  "30 to 39 years"=c("30 to 34 years",
                                                     "35 to 39 years"),
                                  "40 to 49 years"=c("40 to 44 years",
                                                     "45 to 49 years"),
                                  "50 to 64 years"=c("50 to 54 years",
                                                     "55 to 59 years",
                                                     "60 to 64 years"),
                                  "65 years and over"=c("65 to 69 years",
                                                        "70 to 74 years",
                                                        "75 to 79 years",
                                                        "80 to 84 years",
                                                        "85 years and over"))) %>%
  mutate(AGE_GROUP = str_replace_all(AGE_GROUP," ","_")) %>%
  group_by(YEAR, STATE, RACE, SEX, AGE_GROUP) %>%
  summarise(PERC_SUB_POP = sum(PERC_SUB_POP), .groups = "drop") %>%
  unite(col = "VARIABLE", RACE, SEX, AGE_GROUP, sep = "_") %>%
  rename("VALUE"=PERC_SUB_POP)
```

We can indeed check that we have the correct number of levels for `VARIABLE` using the `distinct()` function.

```{r}
 distinct(dem_LOTT, VARIABLE)
```
  
## Combining population Data

We also have population data for each decade that came from wrangling the demographic data.

We again want to combine this data, so let's again make sure that all the different tibbles have the same column names.

```{r}
setequal(colnames(pop_77_79),colnames(pop_80_89))
setequal(colnames(pop_80_89),colnames(pop_90_99))
setequal(colnames(pop_90_99),colnames(pop_00_10))

head(pop_77_79)
head(pop_80_89)
head(pop_90_99)
head(pop_00_10)
```

Looks good!

```{r}
population_data <- bind_rows(pop_77_79,
                             pop_80_89,
                             pop_90_99,
                             pop_00_10)

population_data <- population_data %>%
  mutate(VARIABLE = "Population") %>%
  rename("VALUE" = TOT_POP)
```

We could check that we have 51 values for each year by using the `count()` function of the `dplyr` package.

```{r}
population_data %>%
  count(YEAR)
```

## Police staffing

<details><summary> Click here to see details about how the police staffing data was wrangled. </summary>

OK, now we will wrangle the police staffing data. We want to limit the data to only the years of interest. Then we will also replace NA values with zero for the `male_total_ct` and `female_total_ct` variables using the `replace_na()` function of the `tidyr` package. We will also, use the `across()` function of the `dplyr` package to select and mutate both of these columns in this way. Since both of these variables have `total_ct` in the name and no other variables do, we can use the `contains()` function of the `dplyr` package to specify that we want to use these columns instead of listing both out.

avocado... why not 2010....

```{r}
glimpse(ps_data)

ps_data %<>%
  filter(data_year >= 1977, 
         data_year <= 2014) %>%
mutate(across(.cols =contains("total_ct"), ~replace_na(., 0)))

glimpse(ps_data)
```

Now we can create a new variable called `officer_total` which will be the sum of these variables. We will then keep just this variable as well as the `data_year`, `pub_agency_name`, and `state_abbr`.

```{r}

ps_data %<>%
  mutate(officer_total = male_total_ct + female_total_ct) %>%
  dplyr::select(data_year,
                pub_agency_name,
                state_abbr,
                officer_total)

ps_data
```

Now we also want to get collapse by `pub_agency_name` to get a total count for each year and each state. So we will do this by using the `group_by()` function and grouping by `data_year` and `state_abbr` and using the `summarise()` function to calculate a sum.

```{r}
ps_data %<>%
  group_by(data_year, state_abbr) %>%
  summarise(officer_state_total=sum(officer_total), .groups = "drop")

ps_data
```
And we will check that we have same number of values (the number of years included in the data) for each state.

```{r}

ps_data %>%
  count(state_abbr)  %>% head()

ps_data %>%
  count(state_abbr) %>%
  filter(n != 38) %>%
  dim()
```
Looks like all the states have 38 values.

Notice also that there are some unusual abbreviations in the `state_abbr` variable.

We will remove data for [US terroitories and associated states]( https://www.fs.fed.us/database/feis/format.html){target="_blank"}  


Abbreviation   | Territory and associated states                                                                    
---------- |-------------
**AS**  | American Samoa 
**GM**  | Guam
**CZ**  | Canal Zone
**FS**  | ??Federated States of Micronesia (usually FM)  
**MP**  |  Northern Mariana Islands
**OT**  | ??U.S. Minor Outlying Islands (usually UM)
**PR**  | Puerto Rico 
**VI**  | Virgin Islands


```{r}

state_of_interest_NULL <- c("AS",
                            "GM",
                            "CZ",
                            "FS",
                            "MP",
                            "OT",
                            "PR",
                            "VI")

ps_data <- ps_data %>%
  filter(!(state_abbr %in% state_of_interest_NULL)) 
```
  
Within the `datasets` package that is loaded with R, there is a data set called `state` that contains an object called `state.abb` that has the state abbreviations and `state.name` that has the state names.
We will combine these now to add the state names to our data.

```{r}
state_abb_data <- tibble( "state_abbr" = state.abb, "STATE" =state.name)
head(state_abb_data)
```

One unusual thing about this data is that NE is used for Nebraska to avoid confusions with NB in Canada. 
So we want to replace that using the `str_replace()` function of the `stringr` package

```{r}
state_abb_data %<>%
  mutate(state_abbr = str_replace(string = state_abbr, 
                                pattern = "NE", 
                            replacement = "NB"))
```

#### {.scrollable }
```{r}
# Scroll through the output!
state_abb_data %>% print(n = 50)
```
####


We need to add DC to this. We will use the `add_row()` function of `dplyr` to do this.  We just need to specify values for both of the variables.

```{r}
state_abb_data %<>%
  dplyr::add_row(state_abbr = "DC",
                      STATE = "District of Columbia")
```

Now we will add this to our police staffing data and then remove the `state_abbr` variable, so that we just have state names. We will also 

```{r}
ps_data %<>%
  left_join(state_abb_data, by = "state_abbr") %>%
  dplyr::select(-state_abbr)
ps_data
```

Now we will rename the variables to match those of the other datasets.
```{r}
ps_data %<>%
  rename(YEAR = "data_year",
         VALUE = "officer_state_total") %>%
  mutate(VARIABLE = "officer_state_total")
ps_data
```


We also need to adjust the value to be that of every 100,000 people in the state. To do so we need the population for each state, which luckily we already have. We will slightly modify the population data and create a new tibble that will make it more clear how we are dividing by it.

```{r}
denominator_temp <- population_data %>%
 select(-VARIABLE) %>%
  rename("Population_temp"=VALUE)
head(denominator_temp)

ps_data %<>%
  left_join(denominator_temp, by=c("STATE","YEAR"))
head(ps_data)
```

Avocado not sure why the lag?- could use inside plm function...- conceptually I get why you might want to know how staffing has changed recently... but this does not seem to calculate that...hmmmm

```{r}
ps_data %<>%
  mutate(VALUE = (VALUE * 100000) / Population_temp) %>%
  #mutate(VALUE = lag(VALUE)) %>%
  mutate(VARIABLE = "police_per_100k_lag") %>%
  select(-Population_temp)

ps_data
```

<!-- </details> -->



## Unemployment

The first thing we need to do with the unemployment data is combine the data across the different states.
 We can do that using the `bind_rows()` function of `dplyr` which appends the data together based on the presence of columns with the same name in the different tibbles. We will use the `map_df()` function of the `purrr` package to allow us to do this across each tibble in our list. We will then select just the annual data for each state and year and we will rename our variables to be consistent with some of other data that we are working with. Thus we would like our variables to be `YEAR`, `VALUE` and `VARIABLE` in all caps.
 
```{r}

ue_rate_data <- ue_rate_data %>%
  map_df(bind_rows, .id = "STATE")

head(ue_rate_data)

ue_rate_data <- ue_rate_data %>%
  dplyr::select(STATE, Year, Annual) %>%
  rename("YEAR" = Year,
        "VALUE" = Annual) %>%
  mutate(VARIABLE = "Unemployment_rate")

head(ue_rate_data)
```

## Poverty rate



<details><summary> Click here to see details about how the poverty data was wrangled </summary>

OK, now for wrangling the poverty data. First let's take a look at it. 
```{r}
head(poverty_rate_data)
```

We can see that the column names are actually shifted down below the row with the year. So we will manually make these values the actual column names.

```{r}
colnames(poverty_rate_data) <- c("STATE",
                                 "Total",
                                 "Number",
                                 "Number_se",
                                 "Percent",
                                 "Percent_se")

poverty_rate_data2 <-poverty_rate_data

```

Let's also remove the rows where the column names are listed, like row number 2.

```{r}
poverty_rate_data  %<>%
  filter(STATE != "STATE")
head(poverty_rate_data)
```

We can also see that there are some extra notes at the end of our data. This is why it is a good idea to look at both the head and tail of your data.

```{r}
tail(poverty_rate_data)
```
We can see that the strings for the state for these rows are very long. We can also see that there are rows that just have the year, where the state is only 4 characters long. We will create a new variable called `length_state` based on the number of characters in the `STATE` values. We will use the `str_length()` function of the `stringr` package. We need to use the `map_dbl()` function to apply this to each row of the `STATE` variable. The `map()` function creates a list, whereas the `map_dbl()` function creates a vector of class double. If we were to use `map()` we would need to use `unlist()` and `pull()`.

```{r}

poverty_rate_data %<>%
 mutate(length_state = map_dbl(STATE, str_length))

# Alternatively with map()
#poverty_rate_data %<>%
#mutate(length_state = unlist(map(pull(poverty_rate_data, STATE), str_length)))

poverty_rate_data
```


Great, now let's look at the tail with our new variable `length_state`
```{r}
tail(pull(poverty_rate_data, length_state))

```

```{r}
poverty_rate_data %<>% 
  filter(length_state <100)

tail(poverty_rate_data)
```
Looks good!

Now let's select all the states that are actually year values to create a new variable about the year. We can do so by using the `str_detect()` function of the `stringr` package to look for digits or values of 0-9. This is indicated by using the `"[:digit:]"`.

As you can see in the [RStudio cheatsheet](https://rstudio.com/resources/cheatsheets/){target="_blank"}  about regular expressions this notation indicates any digit between 0 and 9.

```{r}
knitr::include_graphics(here("img", "regex.png"))
```

#### {.scrollable }

```{r}
# Scroll through the output!
poverty_rate_data %>% 
  filter(str_detect(STATE, "[:digit:]")) %>%
  print(n = 51)
```

####

Some of the years (2013 and 2017) are listed twice with a number in parentheses, others are just listed once with a number in parentheses. Looking at the technical documentation, this seems to do with updates to the definition of poverty and to the methods used to estimate poverty levels. See [here](https://www.census.gov/topics/income-poverty/poverty/guidance/poverty-footnotes/cps-historic-footnotes.html){target="_blank"} and [here](https://www2.census.gov/programs-surveys/cps/techdocs/cpsmar19.pdf){target="_blank"} for more information. We will simply select one of the sets of data for 2013 and 2017.

```{r}
poverty_rate_data %>% 
  filter(str_detect(STATE, "2013")) %>%
  filter(str_detect(STATE, "2017"))
```

First let's add the year value to our data. 


There should be consistently data for 51 states (including DC). We can see that sometimes DC is spelled out and sometimes it is not.

#### {.scrollable }
```{r}
### Scroll through the output!
poverty_rate_data %>% 
  filter(str_detect(STATE, "[:alpha:]")) %>%
  distinct(STATE) %>% print(n = 100)

```
####


Now we will replace `"D.C."` with `"District of Columbia"` using `str_replace()`. We can use the `tally()` function of the `dplyr` package to check that we have fewer now.

```{r}
poverty_rate_data %<>% 
mutate(STATE = str_replace(STATE, pattern = "D.C.", 
                              replacement = "District of Columbia" ))

poverty_rate_data %>% 
  filter(str_detect(STATE, "[:alpha:]")) %>%
  distinct(STATE) %>% tally()
```
Great! Now are each of the states occurring as often as the unique year values? We can first check how many year values there are. Then can use the `count()` function of the `dplyr` package to check how often the states are repeated.

```{r}

poverty_rate_data %>% 
  filter(str_detect(STATE, "[:digit:]")) %>%
  tally()
```

There are 41 different sets of data according to year values.

#### {.scrollable }
```{r}
### Scroll through the output!
poverty_rate_data %>% 
  filter(str_detect(STATE, "[:alpha:]")) %>%
  count(STATE) %>% 
  print(n = 51)
```
####

Indeed, looks like each of the states are repeated the same number of times!

Now let's create a new variable `YEAR` that repeats the year values for all of the different states and for the row that has just the year value for a total of 52.

```{r}

year_values <- poverty_rate_data %>% 
  filter(str_detect(STATE, "[:digit:]")) %>%
  distinct(STATE)

  year_values<-rep(pull(year_values, STATE), each = 52)
setequal(length(year_values), length(poverty_rate_data$STATE))
```

Now we will add this to our `poverty_rate_data`. We will also remove the `length_state` variable using the `select()` function of the `dplyr` package and a minus sign before the variable name.

```{r}
poverty_rate_data %<>%
  mutate(year_value = year_values) %>%
  select(-length_state)
```

#### {.scrollable }
```{r}
#Scroll through the output!
poverty_rate_data %>% print(n = 100)
```
####

Looks good! Now we will remove the rows that have just the year values by only preserving those with alpha characters.

```{r}
poverty_rate_data %<>%
    filter(str_detect(STATE, "[:alpha:]"))

```

Now let's remove the older data for 2013 and 2017 which is the data that appears lower in the tibble.

```{r}
poverty_rate_data %<>%
filter(year_value != "2017") %>%
filter(year_value != "2013 (18)")
```


We also want to just keep the first 4 digits of the year_value and create a `YEAR` variable. We need to pull the `year_value` data because `str_sub()` expects a character vector not a tibble.

```{r}
poverty_rate_data %<>%
  mutate(YEAR = str_sub(pull(., year_value), start = 1, end=4))
```

#### {.scrollable }
```{r}
poverty_rate_data 
```
####

Looks good! Now we will just remove the extra variables and rename the variables we want to keep to be similar to our other data.

```{r}
poverty_rate_data %<>%
  dplyr::select(- Number,
                - Number_se,
                - Percent_se,
                - Total,
                - year_value) %>%
  rename("VALUE" = Percent) %>%
  mutate(VARIABLE = "Poverty_rate",
         YEAR = as.numeric(YEAR),
         VALUE = as.numeric(VALUE))
head(poverty_rate_data)
```

Looks great! 

AVOCADO this data is per 1000k? 


From Michael:
```{r, eval = FALSE}

poverty_rate_data <-poverty_rate_data2
notes <- 4

poverty_rate_data2 <- poverty_rate_data[-((dim(poverty_rate_data)[1]-notes+1):dim(poverty_rate_data)[1]),]

states_eq <- 51

extra_col <- 2

rep_rows <- states_eq + extra_col

groups <- (dim(poverty_rate_data)[1])/(rep_rows)

paste(groups - (2018-1980 + 1), "extra groups")

poverty_rate_data$year_group <- rep(1:groups, each=rep_rows)

poverty_rate_data <- poverty_rate_data %>%
  group_by(year_group) %>%
  group_split()

head(poverty_rate_data[[1]])

poverty_rate_data <- poverty_rate_data %>%
  map(~mutate(.,
              row_id = row_number())) %>%
  map(~filter(.,row_id != 2)) %>%
  map(~dplyr::select(.,-row_id))

poverty_rate_data_names <- poverty_rate_data %>%
  sapply(., "[",1,1, drop=TRUE) %>%
  str_replace_all(.,"[:space:]","_")

names(poverty_rate_data) <- poverty_rate_data_names

# Recall 2 extra groups. 
# footnotes available at https://www.census.gov/topics/income-poverty/poverty/guidance/poverty-footnotes/cps-historic-footnotes.html

poverty_rate_data$`2017_(21)` <- NULL

poverty_rate_data$`2013_(19)` <- NULL

poverty_rate_data_names <- poverty_rate_data %>%
  sapply(., "[",1,1, drop=TRUE) %>%
  str_sub(., start = 1, end=4)

names(poverty_rate_data) <- poverty_rate_data_names

poverty_rate_data <- poverty_rate_data %>%
  map_df(bind_rows, .id = "YEAR") %>%
  dplyr::select(-year_group)

poverty_rate_data <- poverty_rate_data %>%
    mutate(n_na = rowSums(is.na(.))) 

# This shows that there is systematic missing values stemmingly *solely* from the rows without poverty data and only a label designating the year
poverty_rate_data %>% 
  group_by(n_na) %>%
  tally()

sapply(poverty_rate_data, class)



colnames(poverty_rate_data)
```

<!-- </details> -->



## Violent crime



<details><summary> Click here to see details about how the violent crime data was wrangled </summary>

The `crime_data` was imported using `read_lines()` and we have some lines that we don't necessarily need. A large amount of the original data is notes at the end of the table. We want to remove these lines. We can determine where they start by searching for the row that contains the first statement of these notes using the `str_which()` function of the `stringr` package. We will subtract one from this as there is a blank line in between.

```{r}
tail(crime_data)
crime_data <- crime_data[-((str_which(crime_data, "The figures shown in this column for the offense of rape were estimated using the legacy UCR definition of rape")-1): length(crime_data))]
#crime_data <- crime_data[-(2143:length(crime_data))]
tail(crime_data)
```

There are lines for each year from 1977 to 2014 as well as four lines about each state and the header information for each state. 
Here you can see what the original data looks like:
```{r}
knitr::include_graphics(here("img", "crime_data.png"))
```

We want to delete the header information and only retain the lines numeric values or state names.  Thus since there are 38 years worth of data for each state and 4 lines for each header, then each state has 42 lines. We want to delete the lines between and including line 2 to 4 for each state. We will save the header information once to use later.
```{r}
head(crime_data)
x <- 2014-1977+1
rep_cycle <- 4 + x
rep_cycle_cut <- 2 + x
colnames_crime<-(crime_data[4])
```

So starting at line 2 and and 3 and 4 we create a sequence of numbers that increase by the number of rows of the length of the individual state data. We can do so using the base `seq()` function. We can take a look at these in order using the base `sort()` function.

```{r}
delete_rows <- c(seq(from = 2,
                       to = length(crime_data),
                       by = rep_cycle),
                 seq(from = 3,
                       to = length(crime_data),
                       by = rep_cycle), 
                 seq(from = 4,
                       to = length(crime_data),
                       by = rep_cycle))
sort(delete_rows)

```
Thus we will delete lines 2, 3, and 4 and then skip 40 lines (to account for the state information for the first state, the lines of information for the 38 years, and then the state information for the next state) and then delete the next 3 consecutive lines and so on. We can indeed see that line 44-46 are what we wish to remove.

```{r}
crime_data[44:46]
```

```{r}
crime_data <- crime_data[-delete_rows]
```


Nice!

Now we can select all the lines that have state information. We can repeat each of these for the 38 years for each state as well as this line that contains the state information by using the base `rep()` function with the `each =` argument. Finally we will remove the `"Estimated crime in "` portion of the string using the `str_remove()` function of the `stringr` package.  We will later combine this with the crime data.

```{r}
state_label_order <-crime_data[str_which(crime_data, "Estimated crime")]
state_label_order

state_label_order <- rep(state_label_order, each = 38)

crime_states <-str_remove(state_label_order, pattern = "Estimated crime in ")
head(crime_states)
```

Nice! Now for the rest of the data. We now need to remove the lines with the state information.

```{r}
crime_data <-crime_data[-str_which(crime_data, "Estimated crime")]
head(crime_data)
tail(crime_data)
```

It appears that the data is comma separated with 8 columns. One of the middle columns often has no values, we need to fill these in with NAs. We can use the `read_csv()` function from the `readr` package to do this. It turns out you don't have to have a file, but you can also use a string our a vector.

```{r}
crime_data_sep <-read_csv(crime_data, col_names = FALSE)
head(crime_data)
```
Nice! Now we just need our column names. Recall that we saved this information. 

```{r}
colnames_crime

colnames(crime_data_sep) <-c("Year",
                             "Population",
                             "Violent_crime_total",
                             "Murder_and_nonnegligent_Manslaughter",
                             "Legacy_rape" ,
                             "Revised_rape", 
                             "Robbery",
                             "Aggravated_assault")
head(crime_data_sep)
```

We also want to combine this with the state information we collected earlier.
We will use the `bind_cols()` function of the `dplyr` package to do this. This requires that the data have the same number of rows.

```{r}
crime_data_sep <-bind_cols(STATE =crime_states, 
          crime_data_sep)

```

Now we will rename the `Viol_crime_count` variable to be `Variable` and we will remove all of the other columns except for `Year`. We will also rename the variables to look like the other datasets.

```{r}
crime_data <- crime_data_sep %>%
  mutate(VARIABLE = "Viol_crime_count") %>%
  rename("VALUE" = Violent_crime_total) %>%
  rename("YEAR" = Year) %>%
  select(YEAR,STATE, VARIABLE, VALUE)

crime_data
```




from Michael:

```{r, eval = FALSE}
crime_data2 <- data.frame(cbind(crime_data2, rep(1:(length(crime_data2)/rep_cycle_cut),each=rep_cycle_cut)))

colnames(crime_data) <- c("String","STATE_GROUP")

crime_data <- crime_data %>%
  group_by(STATE_GROUP) %>%
  group_split()

columns_crime_data <- 8

crime_data <- crime_data %>%
  map(~mutate(.,
               State = case_when(str_detect(String, "Estimated crime in ") ~ substring(String, nchar("Estimated crime in ")+1)),
              row_id = row_number())) %>%
  map(~fill(., State)) %>%
  map(~filter(.,row_id > 2)) %>%
  map(~mutate(.,
              String = paste0(String, ",", State))) %>%
  map(~dplyr::select(.,String)) %>%
  map(~str_split_fixed(.$String,",",columns_crime_data + 1)) %>%
  map(~data.frame(.)) %>%
  map(~rename(.,"YEAR"=X1,
              "Extra_col1"=X2,
              "VC"=X3,
              "Extra_col2"=X4,
              "Extra_col3"=X5,
              "Extra_col4"=X6,
              "Extra_col5"=X7,
              "Extra_col6"=X8,
              "STATE"=X9)) %>%
  map(~dplyr::select(.,-contains("Extra_col"))) %>%
  map(~.x %>% mutate_all(~trimws(.,which = "both"))) %>%
  map_df(bind_rows)

sapply(crime_data, class)

crime_data <- crime_data %>%
  mutate(VARIABLE = "Viol_crime_count") %>%
  rename("VALUE" = VC) %>%
  as.tibble() %>%
  mutate(YEAR = as.numeric(YEAR),
         VALUE = as.numeric(VALUE))
```

<!-- </details> -->

## RTC laws

<details><summary> Click here to see details about how the RTC Law data was wrangled </summary>


The information about the laws for each state are located on page 62 of the [Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"} article, so first we will select just this page. We can print part of the character string for this page using the `utils` `str()` function and the `ncar.max` argument.

```{r}
DAWpaper_p_62 <- DAWpaper[[62]]
str(DAWpaper_p_62, nchar.max = 1000)
```

 We can also use the `cat` function to see the data printed nicely to see what we are going for.
```{r}

cat(DAWpaper_p_62)
```


We can see that this is one continuous character string. We can separate into lines based on the presence of `"\n"` in the string using the `str_split()` function of the `stringr` package. We need to unlist the data first, as the output of `str_split()` is a list. Finally, we can convert it to a tibble using the `as.tibble()` function of the `tibble` package.  We also see that we don't need the first line about the table. We can remove this with the `slice()` function of the `dplyr` package. We can also use this to remove the column names so that we can replace them. Thus we will use `slice(-(1:2))` to remove the first two lines.

So we will split and unlit() the data.
```{r}
p_62 <- DAWpaper_p_62 %>%
    str_split("\n") %>%
    unlist() %>%
    dplyr::as_tibble() %>%
    slice(-(1:2))

head(p_62)
tail(p_62)

```



We also see by looking at the tail that we want to remove the last two lines. One is empty and the other has only 63 characters, which is the line with the page number.

```{r}
p_62 %<>%
  rename(RTC = value)
p_62 %>%
  mutate(RTC = map_chr(RTC, str_length)) %>%tail()

p_62[53,] # physcial page 60
p_62[54,] # empty line
p_62 %<>%
    slice(-c(53:54))
```

Now we will try splitting by spaces. We can show the output withe the `first()` and `nth()` functions of the `dplyr` package.

```{r}
p_62 %>% pull(RTC) %>% map(str_split, pattern = " ") %>% first()
p_62 %>% pull(RTC) %>% map(str_split, pattern = " ") %>% nth( 5)
```


Interesting, we can see that there are lots of spaces between the elements of the table and that they vary by line. For example there are 6 spaces before Alabama and 7 spaces before Alaska.

Overall, that didn't work quite like we expected. 

Recall from the cheatsheet that `"\\s"` indicates a space. There are also ways to specify how many spaces using curly brackets`{}`. 

```{r}
knitr::include_graphics(here("img", "regex.png"))
knitr::include_graphics(here("img", "quantifiers.png"))
```

The spacing appears to vary quite a bit. WE can use the `str_count()` function of the `stringr` package to see how often we have white spaces larger than 5, 10, 15, or 40 spaces.
```{r}
# how often are there white spaces with more than 5 spaces
p_62 %>% 
  pull(RTC) %>% 
  map(str_count, pattern = "\\s{5,}") %>% 
  unlist()
# how often are there white spaces with more than 10 spaces
p_62 %>% 
  pull(RTC) %>% 
  map(str_count, pattern = "\\s{10,}") %>%
  unlist()

# how often are there white spaces with more than 15 spaces
p_62 %>% 
  pull(RTC) %>% 
  map(str_count, pattern = "\\s{15,}") %>%
  unlist()

# how often are there white spaces with more than 40 spaces
p_62 %>% 
  pull(RTC) %>%
  map(str_count, pattern = "\\s{40,}") %>% 
  unlist()
```

Rows with white spaces with more than 40 consecutive spaces is less common. It appears to be the case in the 1st and 5th row. 

If we take a look at those rows we can see that this occurs when we have a missing value.


```{r}
cat(DAWpaper_p_62)
```


So we will replace white spaces with more than 40 consecutive spaces with `NA`. Let's also remove the leading white spaces that varies in front of the state names, as DC does not have any and this could cause a problem later. We will also replace any white spaces of 2 consecutive spaces or more , but less than 15 white spaces with "|" so that we can split the data based on this symbol. Thus we will also put these around the `NA` value that we are using replace the white spaces made of 40+ spaces. 
```{r}

p_62b <-p_62 %>%
  mutate(RTC = str_replace_all(pull(., RTC), "\\s{40,}", "|N/A|")) %>%
  mutate(RTC =str_trim(pull(., RTC), side = "left")) %>%
  mutate(RTC = str_replace_all(pull(., RTC), "\\s{2,15}", "|"))
head(p_62b)
```

Now anytime there is  one or more `"|"` we should have a column break. So now we will split the data by this symbol.
```{r}

p_62b <-pull(p_62b, RTC) %>%
  str_split( "\\|{1,}") 

head(p_62b)
```

Great! Now we want to put our data in tibble format. To do so we need to bind the rows together. We can do so using the base `rbind()` function. We will use this instead of the `bind_rows()` function of `dplyr` because `rbind()` is less restrictive and allows for columns without names. We will use the base `do.call()` function, so that this is performed along each character string within the list of `p_62b` while maintaining the structure. Then we create a tibble out of this. avocado... describe do.call better... maybe do something tidyverse... 

```{r}
p_62 <- as.tibble(do.call(rbind, p_62b))

colnames(p_62) <- c("STATE",
                    "E_Date_RTC",
                    "Frac_Yr_Eff_Yr_Pass",
                    "RTC_Date_SA")

p_62 <- p_62 %>%
  dplyr::select(STATE, RTC_Date_SA) %>%
  rename("RTC_LAW_YEAR"= RTC_Date_SA) %>%
  mutate(RTC_LAW_YEAR = as.numeric(RTC_LAW_YEAR)) %>%
  mutate(RTC_LAW_YEAR = case_when(RTC_LAW_YEAR == 0 ~ Inf,
                              TRUE ~ RTC_LAW_YEAR))

RTC <-p_62
RTC
```

avocado why inf?

from Michael
```{r, eval = FALSE}

p_62b <-pull(p_62b, RTC) %>%
  str_split( "\\|{1,}") %>% as.data.frame()
  unlist()



p_62b <-p_62 %>%
  mutate(RTC = str_replace_all(pull(., RTC), "\\s{40,}", "|N/A|")) %>%
   mutate(RTC = str_replace_all(pull(., RTC), "\\s{2,15}", "|"))

p_62b %<>%
mutate(value = str_split(pull(., value), "\\|{1}"))

read_csv(p_62b$value)  

p_62 <- p_62 %>%
    apply(1,str_replace_all, "\\s{40,}", "|N/A|") %>%
    str_replace_all("\\s{2,15}", "|") %>%
    as.data.frame()

p_62 <- sapply(p_62$., str_split, "\\|{1,}")

sapply(p_62, nchar)

p_62 <- lapply(p_62, function(x) x[nchar(x) > 0]) 

p_62 <- as.data.frame(do.call(rbind, p_62))

rownames(p_62)

rownames(p_62) <- c()

colnames(p_62) <- c("STATE",
                    "E_Date_RTC",
                    "Frac_Yr_Eff_Yr_Pass",
                    "RTC_Date_SA")
sapply(p_62, class)

p_62 <- p_62 %>%
  dplyr::select(STATE, RTC_Date_SA) %>%
  rename("RTC_LAW_YEAR"=RTC_Date_SA) %>%
  mutate(RTC_LAW_YEAR = as.numeric(RTC_LAW_YEAR)) %>%
  mutate(RTC_LAW_YEAR = case_when(RTC_LAW_YEAR == 0 ~ Inf,
                              TRUE ~ RTC_LAW_YEAR))

sapply(p_62, class)

head(p_62)
```

## Joining Data


Now we will join the data from the different data sets together to create a tibble of data for an analysis that will be similar to the data used by [Donohue et al.](https://www.nber.org/papers/w23510.pdf) {target="_blank"} and [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"}.

First we need to check that our data is indeed ready to be joined. We need to make sure that the column names are the same for each dataset that we intend to combine together. 

We will use the `compare_df_cols()` and `compare_df_cols_same()` functions of the janitor package, to ensure that the column names are the same and that the column values are the same type so that the tibbles can be joined by row. 

If they can be joined by row, then `compare_df_cols_same()`  returns the value `TRUE`, while compare_df_cols(), provides a description of the columns.

```{r}
library(janitor)

data_list <-  list(dem_DONOHUE,
                dem_LOTT,
                population_data,
                ue_rate_data,
                poverty_rate_data,
                crime_data,
                ps_data) #police staffing

janitor::compare_df_cols_same(data_list)
janitor::compare_df_cols(data_list)


checkstate <- function(x) { x %<>% distinct(STATE) %>% tally() %>% pull(n) }
map(data_list, checkstate)
checkyear <- function(x) { x %<>% distinct(YEAR) %>% tally() %>% pull(n) }
map(data_list, checkyear)
```

Avocado bind_rows will make `NA` values for the later years beyond 2010 for the datasets that have them... not sure why we are adding these... maybe there is more about that later?

## Donohue, et al.

We will now bind the demographic data that we made for the Donohue-like analysis called `dem_DONOHUE`, as well as all the other datasets that we have wrangled. This is possible because we have the same column names for each dataset. We will also use the `pivot_wider()` function of the `tidyr` package to change the shape of the data. This will make the data have more columns. Each unique value in the column called `VARIABLE` will be used to make new columns. and the values for each will come from the column called `VALUE`.

```{r}
DONOHUE_DF <- bind_rows(dem_DONOHUE,
                        ue_rate_data,
                        poverty_rate_data,
                        crime_data,
                        population_data,
                        ps_data)
head(DONOHUE_DF)
```


```{r}
DONOHUE_DF %<>%
  pivot_wider(names_from = "VARIABLE",
              values_from = "VALUE")

DONOHUE_DF %>%
  slice_sample(n = 10) %>%
  glimpse()
```

We will also add the Right to Carry Law data using the `left_join()` function of the `dplyr` package. Which will place the `DONOHUE_DF` data on the left of the `RTC` data.  Values will be matched by STATE. Then we will create a new variable called `RTC_LAW` using the `mutate()` function and the `case_when()` function of the `dplyr` package  that will have the value `TRUE` if the current year data is equal to or greater than the year that a more permissive RTC law was adopted, otherwise the value will be `FALSE`.

```{r}

head(RTC)

DONOHUE_DF %<>%
  left_join(RTC , by = c("STATE")) %>%
  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,
                              TRUE ~ FALSE))

DONOHUE_DF %>%
  slice_sample(n = 10) %>%
  glimpse()
```

Since we have differing numbers of years for each data set, we can use the `drop_na()` function of the `tidyr` package. to remove years that have incomplete data. Thus any row with NA values will be removed.

For example, we can see that for 1977, although we have most of the data, we do not have the poverty rate. 
```{r}
DONOHUE_DF %>%
  filter(YEAR == 1977) %>%
  head() %>%
  glimpse()

```

Another example, in 2018 we only have information about unemployment rates, poverty rates, and RTC laws.

```{r}
DONOHUE_DF %>%
  filter(YEAR == 2018) %>%
  head() %>%
  glimpse()

```

```{r}
DONOHUE_DF %<>%
 drop_na()

head(DONOHUE_DF) %>% 
  glimpse()
tail(DONOHUE_DF) %>% 
  glimpse()

```

Now we have complete data and the data spans from 1980 to 2010.

```{r}
DONOHUE_DF %>% distinct(YEAR) %>% pull(YEAR)
```

If we include states that had a RTC law adopted before our time span of data, say 1975, then we only have information about crime rates and the other variables of interest after the law was adopted but not before, therefore including these states doesn't really makes sense. Thus, we will drop the data for these states. We can use the `set_diff()` function of the `dplyr` package to see what states are in the `population_data` that contains all the original 51 states (recall this includes the District of Columbia) but are not in the `DONOHUE_DF`. The order matters here. If we did it the other way around with `population_data` listed second, then set_diff would test if there are any states in `Donohue_DF` that are not in `population_data`. As there are none this would result in nothing.

```{r}
baseline_year <- min(DONOHUE_DF$YEAR)
censoring_year <- max(DONOHUE_DF$YEAR)

DONOHUE_DF %<>%
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) %>%
  filter(RTC_LAW_YEAR > TIME_0)

# DONOHUE_DF %<>% 
#   mutate(STATE = as.factor(STATE))
# 
# DONOHUE_DF %>% 
#   pull(STATE) %>% 
#   levels()

setdiff(distinct(population_data, STATE), 
        distinct(DONOHUE_DF, STATE))
```

We will also calculate a violent crime rate relative to the population in that state at that time, now that we have data for both crime count and population.  Will will also calculate the log value of this rate and the population.

```{r}
DONOHUE_DF %<>%
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))

```

From michael:

```{r, eval = FALSE}
 DONOHUE_DF %>%
   count() %>%
   filter(n != 51) %>%
   print(n=dim(.)[1])

 summary(as.factor(DONOHUE_DF$STATE))

 max(DONOHUE_DF$YEAR) - min(DONOHUE_DF$YEAR) + 1

 DONOHUE_DF <- DONOHUE_DF %>%
   mutate(STATE = fct_collapse(STATE, "District of Columbia"=c("District of Columbia","D.C.")))

 summary(as.factor(DONOHUE_DF$STATE))
  
length(levels(DONOHUE_DF$STATE))

DONOHUE_DF <- DONOHUE_DF %>%
  group_by(STATE, YEAR) %>%
  summarise_all(~na.omit(unique(.))) %>%
  ungroup() # This identifies unique observations, coalesces rows according to the grouping variable(s), and gets rid of of units that have incomplete data. This gives returns a dataframe with the most complete information.

summary(as.factor(DONOHUE_DF$STATE)) 

baseline_year <- min(DONOHUE_DF$YEAR)
censoring_year <- max(DONOHUE_DF$YEAR)

# Need to fix this to ensure severe bias is not introduced by prevalent "cases"

DONOHUE_DF <- DONOHUE_DF %>%
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) %>%
  filter(RTC_LAW_YEAR > TIME_0)

DONOHUE_DF <- DONOHUE_DF %>%
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))

summary(droplevels(as.factor(DONOHUE_DF$STATE)))

length(summary(droplevels(as.factor(DONOHUE_DF$STATE))))

```

## Lott and Mustard

We will now bind the demographic data that we made for the Lott-like analysis called `dem_Lott`, as well as all the other datasets that we have wrangled just as we did for the Donohue-like analysis. Again, this is possible because we have the same column names for each dataset. 

```{r}
LOTT_DF <- bind_rows(dem_LOTT,
                     ue_rate_data,
                     poverty_rate_data,
                     crime_data,
                     population_data,
                     ps_data) %>%
  pivot_wider(names_from = "VARIABLE",
              values_from = "VALUE") %>%
  left_join(RTC , by = c("STATE")) %>%
  mutate(RTC_LAW = case_when(YEAR >= RTC_LAW_YEAR ~ TRUE,
                              TRUE ~ FALSE)) %>%
   drop_na()


baseline_year <- min(LOTT_DF$YEAR)
censoring_year <- max(LOTT_DF$YEAR)

LOTT_DF %<>%
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) %>%
  filter(RTC_LAW_YEAR > TIME_0)

setdiff(distinct(population_data, STATE), 
        distinct(LOTT_DF, STATE))

LOTT_DF %<>%
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))

```
Let's see how the data compares:

We will check the dimensions of each using the base `dim()` function
```{r}
dim(LOTT_DF)
dim(DONOHUE_DF)
```

As expected the `Lott_DF` is 30 columns larger, due to the 30 additional demographic variables. We can check those now as well.

```{r}
LOTT_DF %>%
   colnames()

DONOHUE_DF %>%
   colnames()
```

Lastly, we will check that the `YEAR` values are the same. We can use the `setequal()` function of the `dplyr` package to see if the values are the same. 

```{r}
setequal(DONOHUE_DF %>% distinct(YEAR),
          LOTT_DF %>% distinct(YEAR))
```


Looks as expected! we have 

from Michael:
```{r, eval = FALSE}
LOTT_DF %>%
  group_by(YEAR) %>%
  tally() %>%
  filter(n != 51) %>%
  print(n=dim(.)[1])

summary(as.factor(LOTT_DF$STATE))

max(LOTT_DF$YEAR) - min(LOTT_DF$YEAR) + 1

LOTT_DF <- LOTT_DF %>%
  mutate(STATE = fct_collapse(STATE, "District of Columbia" = c("District of Columbia","D.C.")))

summary(as.factor(LOTT_DF$STATE))
  
length(levels(LOTT_DF$STATE))

LOTT_DF <- LOTT_DF %>%
  group_by(STATE, YEAR) %>%
  summarise_all(~na.omit(unique(.))) %>%
  ungroup() # This identifies unique observations, coalesces rows according to the grouping variable(s), and gets rid of of units that have incomplete data. This gives returns a dataframe with the most complete information.

summary(as.factor(LOTT_DF$STATE)) 

baseline_year <- min(LOTT_DF$YEAR)
censoring_year <- max(LOTT_DF$YEAR)

# Need to fix this to ensure severe bias is not introduced by prevalent "cases"

LOTT_DF <- LOTT_DF %>%
  mutate(TIME_0 = baseline_year,
         TIME_INF = censoring_year) %>%
  filter(RTC_LAW_YEAR > TIME_0)

LOTT_DF <- LOTT_DF %>%
  mutate(Viol_crime_rate_1k = (Viol_crime_count*1000)/Population,
         Viol_crime_rate_1k_log = log(Viol_crime_rate_1k),
         Population_log = log(Population))

summary(droplevels(as.factor(LOTT_DF$STATE)))

length(summary(droplevels(as.factor(LOTT_DF$STATE))))
```


```{r, echo = FALSE, eval = FALSE}
save(LOTT_DF, DONOHUE_DF, file = here::here("data", "Wrangled_data.rda"))
```


# **Data Exploration**
***

```{r, echo = FALSE, eval = TRUE}
# For instructors that want to start here, we will load the data
load( file = here::here("data", "Wrangled_data.rda"))
```

 Let's do some quick visualizations to get a sense of our outcome of interest, the violent crime data. 
 
First we will plot the rate of violent crime over time to get a sense of the general trend.

To do so we need to summarise the data for each year across all of the states. 
Thus we will use the `group_by()` function and the `summarise()` functions to calculate an overall sum of violent crime relative to the population for each year. 

Then we will use the `ggplot2` package to plot the data. The first step in creating a plot with this package is to use the `ggplot()` function and the `aes()` argument to specify what data should be plotted on the x-axis and what data should be plotted in on the y-axis. Then we select what type of plot we would like to make using one of the `geom_*()` functions. Please see [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/){target="_blank"}  for more details.

We can use the `scale_x_continuous()` and `scale_y_continuous()` functions to  modify the axis labels.

The `labs()` function can be used to add labels to the plot, while the `theme()` function allows for manipulation of the details of the labels, like size and angle. 

All of these functions are part of the `ggplot2` package.
 
```{r}
 
 DONOHUE_DF %>%
  group_by(YEAR) %>%
  summarise(Viol_crime_count = sum(Viol_crime_count),
                  Population = sum(Population),
                    .groups = "drop") %>%
  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count*100000)/Population)) %>%
  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log)) +
  geom_line() +
  scale_x_continuous(breaks = seq(1980, 2010, by = 1),
                     limits = c(1980, 2010),
                     labels = c(seq(1980, 2010, by = 1))) +
  scale_y_continuous(breaks = seq(5.75, 6.75, by = 0.25),
                     limits = c(5.75, 6.75)) +
  labs(title = "Crime rates fluctuate over time",
       x = "Year",
       y = "ln(violent crimes per 100,000 people)") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 90))
```
Interesting! It appears that there was an overall national peak in violent crime in the early 1990s that has since then declined.  


Now let's take a look at each state.

We will use the `ggrepel` package to add text to the plot using the `geom_text_repel()` function. This is especially useful when there is a lot of text, as this function reduces the overlap of text labels. Again see [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/){target="_blank"}  for more details on how to add labels to elements of plots.
 
```{r}

DONOHUE_DF %>%
  mutate(Viol_crime_rate_100k_log = log((Viol_crime_count*100000)/Population)) %>%
  ggplot(aes(x = YEAR, y = Viol_crime_rate_100k_log, color = STATE)) +
  geom_point(size = 0.5) +
  geom_line(aes(group=STATE),
            size = 0.5,
            show.legend = FALSE) +
  geom_text_repel(data = DONOHUE_DF %>%
              mutate(Viol_crime_rate_100k_log = log((Viol_crime_count*100000)/Population)) %>%
              filter(YEAR == last(YEAR)),
            aes(label = STATE,
                x = YEAR,
                y = Viol_crime_rate_100k_log),
            size = 3,
            alpha = 1,
            nudge_x = 10,
            direction = "y",
            hjust = 1,
            vjust = 1,
            segment.size = 0.25,
            segment.alpha = 0.25,
            force = 1,
            max.iter = 9999) +
  guides(color = FALSE) +
  scale_x_continuous(breaks = seq(1980, 2015, by = 1),
                     limits = c(1980, 2015),
                     labels = c(seq(1980, 2010, by = 1), rep("", 5))) +
  scale_y_continuous(breaks = seq(3.5, 8.5, by = 0.5),
                     limits = c(3.5, 8.5)) +
  labs(title = "States have different levels of crime",
       x = "Year",
       y = "ln(violent crimes per 100,000 people)") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 90))


```

Looks like the crime rages vary quite a bit from one state to another. Some states show increased crime over time while others show decreased crime. 


Now let's take a closer look at some of our other variables.

We can also use the `vis_miss()` function of the `naniar` package to confirm that there are no missing values.

```{r}
DONOHUE_DF %>%
  naniar::vis_miss()

```

Looks like no missing data! 

```{r}
LOTT_DF %>%
  naniar::vis_miss()

```
Same for the `LOTT_DF`.

We can use the `skim()` of the `skimr` package to get a better sense of the data. This also shows missingingness, as well as standard deviations, spread, and means for our data. Also notice that there is a histogram of each variable.

```{r}
skimr::skim(DONOHUE_DF)
skimr::skim(LOTT_DF)

```
We can see from this function, that we have the number of variables of the class types that we expect for each tibble. We can also see that the mean of the variables that should be the same for each tibble are in fact the same. 
We can also tell that the values for the variables are in general what we would expect.

# **Data Analysis**
***

## Donohue, et al.

OK! We are now ready to start analyzing our data!. 

We have what is called [panel data](https://en.wikipedia.org/wiki/Panel_data){target="_blank"} . This is a special type of longitudinal data. Longitudinal data are data measurements taken over time. Panel data are data repeatedly measured for for multiple panel members or individuals over time. This is in contrast with time series data, which measures one individual over time and cross sectional data, which measures multiple individuals at one point in time.  In other words, panel data is a combination of both, in this case we measure multiple individuals over multiple time periods.  In our case, we have measurements of violent crime and other variables for each state over many years. Therefore we are using measurements about the same states over time.  

In a panel analysis there are $N$ individual panel members and $T$ time points.  

There are two types of panels:  
1. **Balanced** - At each time point ($T$), there are data points for each individual($N$). 

Time Points ($T$)  | Individuals ($N$)                                                                      
---------- |-------------
1977  | Nevada 
1977  | Alabama
1977  | Kansas
1978 | Nevada
1978 | Alabama
1978  | Kansas
1979 | Nevada
1979 | Alabama
1979 | Kansas

2. **Unbalanced** - There may be data points missing for some individuals ($N$) at some time points ($T$).

Time Points ($T$)  | Individuals ($N$)                                                                         
---------- |-------------
1977  | Nevada 
1977  | Alabama
1978 | Nevada
1978 | Alabama
1979 | Nevada
1979 | Alabama
1979 | Kansas


Overall in a a balanced panel, we have $n$ observations, where $n = N*T$.  

In an unbalanced panel, the number of observations is less than $N*T$.


In our case we have:  
$N$ = 45 states (recall that we removed those who had adopted a RTC law before 1980)  
$T$ =  31 years (1980 - 2010)  

In every year we have measurements for each state (as we just saw above), thus our panel is balanced.

So, our total observations $n = 45*31$, thus $n$ = `r 45*31`.  



We will be performing a **panel linear regression model analysis**. 


In such an analysis we will model our data according to this generic model:

$$Y_{it}=β_{0}+β_{1}X_{1it}+...+β_{K}X_{Kit}$$

Where $i$ is the individual dimension (in our case individual states) and $t$ is the time dimension.

Some explanatory/independent variables or regressors $X_{it}$ will vary across individuals and time, while others will be fixed across the time of the study (or don't change over time), while others still will be fixed across individuals but vary across time periods.


There are three general sub-types of [panel regression analysis](https://en.wikipedia.org/wiki/Panel_analysis){target="_blank"}.

Overall, they assume that the different individuals are independent, however the same data for the same individual may be correlated across time.

The main difference between the three sub-types are the assumptions about unobserved differences between individuals.

avocado: update formulas like this? https://bookdown.org/ccolonescu/RPoE4/panel-data-models.html
pooled:
like this for fixed effect: 
random: $$yit=β⎯⎯⎯1+β2x2it+νit$$



1) **independently pooled panels**  - assumes that there are no individual effects that are independent of time and also no effect of time on all the individuals. In other words, the independent variables are not correlated with any error term. This is essentially an ordinary least squares linear regression. This type of panel regression makes the most assumptions and is therefore typically not used for panel data.

 $$Y_{it}=\beta_{0}+\beta_{1}x_{1it}+...+\beta__KX_{Kit} + e_{it}$$
Where the intercept $\beta_0it=\beta_0$for all $i,t$ and slope $\beta_kit=\beta_k$ for all $i,t$.

2) **fixed effects** - assumes that there are unknown or unobserved unique aspects about the individuals  or heterogeneity among individuals $B_0i$ that are not explained by the independent variables but influence the outcome variable of interest. They do not vary with time or in other words are fixed over time but may be **correlated** with independent variables $X_{it}$.  

In this case the intercept can be different for each individual $\beta_{0i}$, but the slope is the same across all the individuals.


These individual $a_i$ effects can be correlated with the independent variables $X$.

$$Y_{it}=\beta_{0i}+\beta_{1}X_{1it}+...\beta_{k}X_{kit}+e_it$$
This type of panel regression makes the least assumptions.


3) **random effects** - assumes that there are unknown or unobserved unique qualities about the individuals that influences the outcome variable of interest that are **not correlated** with the independent variables. 

Thus, the random effects model actually makes **more assumptions** than the fixed effect model.

$$Y_{it} =\beta_0 X_{it} +\beta_{1}X_{1it}+... \beta_k X_{kt} + e_{t}$$
So each individual has the same slope and the same overall error term ($\beta + e_{t}$). 


See [here](https://www.bauer.uh.edu/rsusmel/phd/ec1-15.pdf) and [here](https://sites.google.com/site/econometricsacademy/econometrics-models/panel-data-models) and [here](https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.htm) for more information about these different models.

We will be performing a fixed effect panel regression analysis, as we do in fact think that some of the unobserved qualities about the different states that may be correlated with some of our independent variables. For example, the level of economic opportunity might be an unobserved feature about the states that influences violent crime rate and would be possibly correlated with poverty rate and unemployment. 


To perform our analysis we will be using the `plm` package. This stands for Panel Linear Model.

We need to use a special type of data to use this package, called a `pdata.frame` which is short for panel data frame. This allows us to specify that we are using panel data and what the panel structure looks like. 

We need to indicate variable should be used to identify the individuals in our panel, and what variable should be used to identify the time periods in our panel. In our case the `STATE` variable identifies the individuals and the `YEAR` variable identifies the time periods.

We can specify this structure using the `pdata.frame()` function of the `plm` package, by using the `index` argument, where the individual variable is specified first followed by the time variable, like so: `index=c("Individual_Variable_NAME", "Time_Period_Variable_NAME").

```{r}
d_panel_DONOHUE <- pdata.frame(DONOHUE_DF, index=c("STATE", "YEAR"))

class(d_panel_DONOHUE)

slice_head(d_panel_DONOHUE, n = 3)
```

Indeed we have now created a pdata.frame object and we can see that the row names show the individual states and time period years.

OK, now we are ready to run our panel linear model on our panel data frame. 

To do so we will use the `plm()` function and we will specify the formula for our model, where the dependent variable `Viol_crime_rate_1k_log` will be on the left of our `~` sign and all of the independent variables will be listed on the right with `+` signs in between each.

We also need to specify what type of `effect` we would like to model and what type of `model` we would like to use.

There are three main options for the `effect` argument:
1) individual  - model for the effect of individual identity
2) time - model for the effect of time
3) twoways - meaning modeling for the effect of both individual identity and time  

There are four main options for the `model` argument:  
1) pooling - standard pooled ordinary least squares regression model  
2) within - fixed effects model (variation between individuals is ignored, model compares individuals to themselves at different periods of time)  
3) between - fixed effects model (variation within individuals from one time point to another is ignored, model compares different individuals at each point of time)  
4) random - random effects (each state has a different intercept but force it to follow a normal distribution - requires more assumptions)

Typically it is best to think about what you are trying to evaluate with your data in trying to choose how to model your data. However, there are also some tests that can help to assess this which we will briefly cover.

We are interested in how violence in each state varied over time, thus we are interested in within `STATE`variation, so we will perform our plm with the `model = within` argument to perform this particular type of fixed effects model. 

We also speculate that there is an effect of individual `STATE` identity and time on violent crime rate. In other words, we expect some states to have high rates of crime, and others to have low rates of crime. We also expect crime to change over time. 

If we were to perform this type of analysis we would use the `effect = "twoways"` argument in our `plm()` function like so:

```{r}
DONOHUE_OUTPUT <- plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                        effect = "twoways",
                        model = "within",
                        data = d_panel_DONOHUE)
```

To see the results we can use the base `summary()` function. We can view this output in tidy format using the `tidy()` function of the `broom` package.

avocado I think the `analysis` variable is a label for plots

```{r}
summary(DONOHUE_OUTPUT)

DONOHUE_OUTPUT_TIDY <- tidy(DONOHUE_OUTPUT, conf.int = 0.95)

DONOHUE_OUTPUT_TIDY

DONOHUE_OUTPUT_TIDY$Analysis <- "Analysis 1"
```

We will now perform a test to determine if we could have simply used a pooled model. This test evaluates if the coefficients (including the intercepts) are equal.  To perform this test we will use the `pooltest()` function of the `plm` package to compare the pooled model to the fixed effect within model.

```{r}
pooltest(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag, 
         data = d_panel_DONOHUE, 
         model = "within")

```

The `p-value` is less than a significance threshold of .05, thus we reject the null that our coefficients are all equal. Thus the `within` fixed effects model fit the data better.


We can also perform a test to evaluate if there is indeed an individual effect and a time effect in our model.

We can use the `plmtest()` function of the `plm` package. This performs a Lagrange Multiplier Test. To do so, we need to get the output for a simple pooled model.

```{r}

DONOHUE_pool_model <-plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                        model = "pooling",
                        data=d_panel_DONOHUE)

plmtest(DONOHUE_pool_model, effect = c("twoways"))

```

Again, the p-value is much smaller than the significance threshold of < 0.05. Therefore we reject the null that there are no effects, and we can feel confident with proceeding with a twoway effect model.





There is one more test that we could perform. To test if using a random effect model would be more appropriate compared to the fixed effect model, one could use the [Hausmen test](https://en.wikipedia.org/wiki/Durbin%E2%80%93Wu%E2%80%93Hausman_test){target="_blank"} for this (also called the Durbin-Wu-Hausman test). This can be implemented using the `phtest()` function of the `plm` package.

```{r}

DONOHUE_random_model <-  plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                      effect = "twoways",
                      model = "random",
                      data=d_panel_DONOHUE)

DONOHUE_within_model <-  plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                       effect = "twoways",
                       model = "within",
                       data=d_panel_DONOHUE)

phtest(DONOHUE_within_model, DONOHUE_random_model)

```

This test evaluates if there are errors $u_i$ that are correlated with any of the independent variables. 

We reject the null hypothesis, that there are no inconsistencies, and are confirmed in our plan to used a fixed effects model.



For more information on these tests and this package, see [here](https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html){target="_blank"}  and [here](http://www.princeton.edu/~otorres/Panel101R.pdf){target="_blank"}.

avocado- stephanie- what do you think about referencing these slides from Princeton?


A final note about the fixed and random effects terminology and how this is slightly different than other definitions:

According to the documentation for the [PLM package](https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html):
>The fixed/random effects terminology in econometrics is often recognized to be misleading, as both are treated as random variates in modern econometrics (see, e.g., Wooldridge (2002) 10.2.1). It has been recognized since Mundlak’s classic paper (Mundlak (1978)) that the fundamental issue is whether the unobserved effects are correlated with the regressors or not. In this last case, they can safely be left in the error term, and the serial correlation they induce is cared for by means of appropriate GLS transformations. On the contrary, in the case of correlation, “fixed effects” methods such as least squares dummy variables or time-demeaning are needed, which explicitly, although inconsistently27, estimate a group– (or time–) invariant additional parameter for each group (or time period).

>Thus, from the point of view of model specification, having fixed effects in an econometric model has the meaning of allowing the intercept to vary with group, or time, or both, while the other parameters are generally still assumed to be homogeneous. Having random effects means having a group– (or time–, or both) specific component in the error term.

>In the mixed models literature, on the contrary, fixed effect indicates a parameter that is assumed constant, while random effects are parameters that vary randomly around zero according to a joint multivariate normal distribution.


## Lott and Mustard

Ok, now we will do the same for the Lott-like analysis. In this case we would have a very large formula to write. So instead, we can use the `as.formula()` function of the `stats` package and the base `paste()` function to combine all of our explanatory variables into one formula without making a mistake. First we will create an object where we select only for the explanatory variables. 

avocado I tried using glue or dplyr::collapse to make this tidyverse but this didn't work

```{r}
LOTT_variables <- LOTT_DF %>%
  dplyr::select(RTC_LAW,
                contains(c("White","Black","Other")),
                Unemployment_rate,
                Poverty_rate,
                Population_log,
                police_per_100k_lag) %>%
  colnames()


LOTT_fmla <- as.formula(paste("Viol_crime_rate_1k_log ~",
                              paste(LOTT_variables, collapse = " + ")
                              )
                        )

LOTT_fmla

```

That is quite the formula!

OK, now again we will make a panel data frame and we will perform fit a fixed effect two-way model for time and individuals (`STATE`) with this data as well.

```{r}

d_panel_LOTT <- pdata.frame(LOTT_DF, index=c("STATE", "YEAR"))

LOTT_OUTPUT <- plm(LOTT_fmla,
                      model = "within",
                     effect = "twoways",
                       data = d_panel_LOTT)

summary(LOTT_OUTPUT)

LOTT_OUTPUT_TIDY <- tidy(LOTT_OUTPUT, conf.int = 0.95)

LOTT_OUTPUT_TIDY$Analysis <- "Analysis 2"
```


## RTC coefficient comparison

Now let's make a plot to compare the coefficient estimate for the Right-to-carry law adoption variable in each model.

First we will combine model fit information for this coefficient for each model.

```{r}
comparing_analyses <- DONOHUE_OUTPUT_TIDY %>%
  bind_rows(LOTT_OUTPUT_TIDY) %>%
  filter(term == "RTC_LAWTRUE")

comparing_analyses
```

We can see that for the first analysis (similar to the [Donohue et al.](https://www.nber.org/papers/w23510.pdf) {target="_blank"} study) the coefficient estimate for the presence of a permissive Right-to-carry law is positive, while for the second analysis (similar to the [Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"} study) the coefficient estimate is negative. Thus in the first analysis we could conclude that the effect of adopting permissive right-to-carry laws  may be associated with increases in violent crime (although this was not a significant result (in contrast with the real [Donohue et al.](https://www.nber.org/papers/w23510.pdf) {target="_blank"} study )); while in the other analysis we could conclude that the laws may be associated with decreases in violent crime.

Let's make a plot of this finding. We will show error bars for the coefficient estimates for both analyses using the `geom_errorbar()` function of the `ggplot2` package. This requires specifying the minimum and maximum for our error bar, which in our case we would like to be the low and high values of our confidence intervals for the coefficient estimates. We will also add a horizontal line at y = 0 using the `geom_hline()` function of the `ggplot2` package. 

Finally we will add arrows to emphasize the difference in the direction of the findings using the `geom_segment()` function of the `ggplot2` package. Using the `arrow()` function, we can specify details about the arrow we would like to add.

```{r}
comparing_analyses_plot <- ggplot(comparing_analyses) + 
  geom_point(aes(x = Analysis, y = estimate)) +
  geom_errorbar(aes(x = Analysis, ymin = conf.low, ymax = conf.high), width = 0.25) + 
  geom_hline(yintercept = 0, color = "red") +
  scale_y_continuous(breaks = seq(-0.2, 0.2, by = 0.05),
                     labels = seq(-0.2, 0.2, by = 0.05),
                     limits = c(-0.2,0.2)) +
  geom_segment(aes(x = 1, y = 0.125, xend = 1, yend = 0.175),
               arrow = arrow(angle = 45, ends = "last", type = "open"),
               size = 2,
               color = "green",
               lineend = "butt",
               linejoin = "mitre") +
  geom_segment(aes(x = 2, y = -0.125, xend = 2, yend = -0.175),
               arrow = arrow(angle = 45, ends = "last", type = "open"),
               size = 2,
               color = "red",
               lineend = "butt",
               linejoin = "mitre") +
  theme_minimal() + 
  theme(axis.title.x = element_blank(),
        axis.text = element_text(size = 8, color = "black")) +
  labs(title = "Effect estimate on ln(violent crimes per 100,000 people)",
       y = "  Effect estimate (95% CI)")

comparing_analyses_plot
```

We can see that most of the possible range of values for the estimate in analysis 1 are positive, while they are all negative for analysis 2.

# **Multicollinearity analysis**
***

How did the above happen?

The analysis data frames are very similar yet rendered very different results. 

Recall that the only difference is the number of demographic variables. The number of rows or observations is the same. We can use the `all_equal()` function of the `dplyr` package to compare the number of columns of our Donohue-like data and our Lott-like data.

```{r}
all_equal(target = DONOHUE_DF,
          current = LOTT_DF,
          ignore_col_order = TRUE,
          ignore_row_order = TRUE)
```

Using the base `dim()` function we can also look at the number of rows for each and see that the number of observations is the same for both datasets.

```{r}
dim(DONOHUE_DF)[1]
dim(LOTT_DF)[1]
```

The only difference between the two data frames rests in how the demographic variables were parameterized.

```{r}
DONOHUE_DF %>%
  dplyr::select(contains("years")) %>%
  colnames()

LOTT_DF %>%
  dplyr::select(contains("years")) %>%
  colnames()
```

Clearly, this had an effect on the results of the analysis. 

Let's explore how this occurred. 

When seemingly independent variables are highly related to one another, the relationships estimated in an analysis may be distorted. 

In regression analysis, this distortion is often a by-product of a violation of the independence assumption. This distortion, if large enough, can impact statistical inference. 

The phenomena called multicollinearity occurs when independent variables are highly related to one another

There are several ways we can diagnose multicollinearity.

### Correlation

One way we can evaluate the relationships between variables is by examining the correlation between variable pairs.

<style>
div.blue { background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

It is important to note that multicollinearity and correlation are not one and the same. [Correlation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5079093/) can be thought of as the strength of a linear relationship between variables. On the other hand, collinearity can be thought of as two independent variables having a linear relationship or association. Multicollinearity can be thought of as collinearity among multiple (3+) regressors (independent variables) in a regression analysis, which can occur when regressors are highly correlated.

</div>


According to [Wikipedia](https://en.wikipedia.org/wiki/Multicollinearity){target="_blank"}:

> multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. 

Thus collinearity describes linear prediction or association. Often those variables will be highly correlated.

The issue with this in linear regression, is that linear regression aims to determine how a one unit change in a regressor influences a one unit change in the dependent variable. In fact, this is what the coefficient estimates aim to tell us for each regressor.

However, if our regressors are also linearly related, than it becomes difficult to determine the effect of each regressor on the dependent variable and multicollinearity can cause instability in the estimation of [coefficient estimates](https://statisticsbyjim.com/glossary/regression-coefficient/){target="_blank"}, making them unreliable. Coefficients may be inflated, deflated, or their signs may change.

avocado... what if the regressors have a nonlinear relationship??


avocado: I really liked these explanations - not sure if we want to include these as resources:
https://blog.clairvoyantsoft.com/correlation-and-collinearity-how-they-can-make-or-break-a-model-9135fbe6936a

https://medium.com/swlh/multicollinearity-and-variance-inflation-factor-bc74af36b1c9

#### Scatter plots

One of the ways to diagnose multicollinearity in a regression model is to first see if there are regressors that are highly correlated. If so, this suggests that we should investigate further to see if these variables are in fact linearly predicting one another.

One way to look at correlation across pairs of variables is to use the `ggpairs()` function of the `GGally` package.

```{r}
colnames(DONOHUE_DF)

DONOHUE_DF %>% 
  dplyr::select(RTC_LAW,
                Viol_crime_rate_1k_log,
                Unemployment_rate,
                Poverty_rate,
                Population_log) %>% 
  ggpairs(.,
          columns = c(2:5),
          lower = list(continuous = wrap("smooth_loess",
                                         color = "red",
                                         alpha = 0.5,
                                         size = 0.1)))
```
We can see that for the non-demographic variables, there is very little correlation between the pairs of variables. Only the unemployment rate and the poverty rate show relatively strong correlation, as one might expect. 


#### Heatmaps

Another way to look at correlation if we have many variables is to use heatmaps.

Let's to this now for the demographic variables for each analysis.

The `ggcorrplot()` function of the `ggpcorplot` package is one way to create such a heatmap.

This requires first calculating the correlation values using the `cor()` function of the `stats` package.

We will add a legend title to include $\rho$ by using the base `expression()` function, which will convert the written for of `"rho"` to the Greek symbol.

```{r}
cor_DONOHUE_dem <- cor(DONOHUE_DF %>% dplyr::select(contains("_years")))

corr_mat_DONOHUE <- ggcorrplot(cor_DONOHUE_dem,
           tl.cex = 6,
           hc.order = TRUE,
           colors = c("red",
                      "white",
                      "red"),
           outline.color = "transparent",
           title = "Correlation Matrix, Analysis 1",
           legend.title = expression(rho))


corr_mat_DONOHUE

cor_LOTT_dem <- cor(LOTT_DF %>% dplyr::select(contains("_years")))

corr_mat_LOTT <- ggcorrplot(cor_LOTT_dem,
           tl.cex = 6,
           hc.order = TRUE,
           colors = c("red",
                      "white",
                      "red"),
           outline.color = "transparent",
           title = "Correlation Matrix, Analysis 2",
           legend.title = expression(rho))

corr_mat_LOTT
```
We can see that many of the demographic variables are highly correlated with one another. 


The presence of correlation between variables suggests that we might have multicollinearity. However it does not necessarily mean that we do. So how can we assess this?


### Coefficient estimate instability

One way to look at the possible influence of multicollinearity is to look at the stability of the coefficient estimates.

We will focus on the `RTC_LAW` variable coefficient estimate, as this is of particular interest in our case.

To do so we will perform a process called [resampling](https://en.wikipedia.org/wiki/Resampling_(statistics)){target="_blank"}. This involves performing multiple iterations of our analysis, but with only a subset or sub-sample of our original data.  In our case we will remove **one** observation and see if that changes our coefficient estimate results. 


To do this we will use some functions in the `rsample` package which is very useful for splitting data in various ways.

We will use the `loo_cv()` function which stands for **leave one out** [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics){target="_blank"}. This will allow us to split our data into every possible subset where a unique observation is left out of the data.

This function will however only prepare the data to be split.

To actually get the remaining data after the removal of the observation that is left out when need to use a function called `training()`. This is because these functions are often used for in machine learning applications where the data is split between a larger training set and a smaller testing set. Thus we want the larger $n-1$ subset, as opposed to the single value that is removed, (which we could get with the `testing()` function)


<details> <summary> Click here to see an example of how this works. </summary>

First we will make a toy dataset that is very simple called test using the `tibble()` function of the `tidyr` package:
```{r}
test <-tidyr::tibble(x = c(1,2,3))
test
```

Now we will use the `loo_cv()` to create leave one out splits:
```{r}
test_samples <- test %>% loo_cv()
test_samples
```

We can take a look at one individual split using the `pull()` function:

```{r}
pull(test_samples, splits)
```

Here you can see that 2 values are intended for the training set (also called Analysis set), 1 value is intended for the testing set (also called Assessment set), and 3 values were present initially.

Now we will use the `training()` function to get the data without the observation that is set aside. Here is the data for the first subset:
```{r}
training(pull(test_samples, splits)[[1]])
```

Now we will use the `map()` function of `purrr` to get all possible `training` subset of the data.
```{r}

test_subsets <- map(pull(test_samples, splits), training)
test_subsets
```

We can see that there are 3 possible subsets that leave one value out. All 3 possible subsets are created using this method. This method will always create the same number of subsets as there are unique values or rows in the data.

</details>


Now we will use this method with the data from our Donohue-like analysis, since this data has `dim(d_panel_DONOHUE)[1]` rows, `dim(d_panel_DONOHUE)[1]` subsets will be created that leave out one row.

First we will create the splits using the `loo_cv()` function:
```{r}
set.seed(124)
DONOHUE_splits <- d_panel_DONOHUE %>% loo_cv()
DONOHUE_splits
```

Now we will use the `training()` function to select the remaining data without the value that was removed for each split:

```{r}
# To get all the data subsets
DONOHUE_subsets <-map(pull(DONOHUE_splits, splits), training)

glimpse(DONOHUE_subsets[[1]])
length(DONOHUE_subsets)
```

As expected the first subset has 1,394 rows and there are 1395 subsets.

Let's see what observation was left out in the first subset:

```{r}
d_panel_DONOHUE %>%
  filter(! rownames(d_panel_DONOHUE)  %in% rownames(DONOHUE_subsets[[1]]))

# Another way to check is to use:
DONOHUE_removed <-map(pull(DONOHUE_splits, splits), testing)

DONOHUE_removed[[1]]
```

It looks like the Texas data from 1988 was removed from the first split.


OK, so now let's fit our panel regression on the first subset of data like we did previously. Note that this causes our data to be an unbalanced panel.
This does not require any adjustment to the code to model the data, but you will notice that the output will now say "unbalanced".

```{r}
 subset_1_result<-plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                        data = DONOHUE_subsets[[1]],
                       index = c("STATE","YEAR"),
                       model = "within",
                      effect = "twoways")
summary(subset_1_result)
```

Indeed, we can see that now we have an unbalanced panel with N = 1394 observations instead of 1395, as expected.

Now that we have our subsets, we want to write a function to fit a panel regression using `plm()`on each subsets. See [this case study](https://opencasestudies.github.io/ocs-bloomberg-vaping-case-study/){target="_blank"}  for more information on writing functions.

```{r}
fit_nls_on_bootstrap_DONOHUE <- function(subset){
                    plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
                        data = data.frame(subset),
                       index = c("STATE","YEAR"),
                       model = "within",
                      effect = "twoways")
}

```

Now we can apply this function to each of our subsets simultaneously using the `map()` function of the `purrr` package.  

```{r, eval = FALSE}

subsets_models_DONOHUE <- map(DONOHUE_subsets, fit_nls_on_bootstrap_DONOHUE)

subsets_models_DONOHUE <- subsets_models_DONOHUE %>%
  map(tidy)

```

```{r, eval = FALSE, echo = FALSE}
save(subsets_models_DONOHUE, 
     file = here::here("data", "DONOHUE_simulations.rda"))
```

```{r, echo = FALSE}
load(here::here("data", "DONOHUE_simulations.rda"))
```



Great! Now we want to do the same thing for the Lott data.

```{r}
set.seed(124)
LOTT_splits <- d_panel_LOTT %>% loo_cv()

# To get all the data subsets:
LOTT_subsets <-map(pull(LOTT_splits, splits), training)
```


We need to create a different function to fit the data to account for the larger number of demographic variables. We will use the formula that we made previously.

```{r}
fit_nls_on_bootstrap_LOTT <- function(split){
  plm(LOTT_fmla,
      data = data.frame(split),
      index = c("STATE","YEAR"),
      model = "within",
      effect = "twoways")
}
```



```{r, eval = FALSE}
subsets_models_LOTT <- map(LOTT_subsets, fit_nls_on_bootstrap_LOTT)

subsets_models_LOTT <- subsets_models_LOTT %>%
  map(tidy)
```


```{r, eval = FALSE, echo = FALSE}
save(subsets_models_LOTT, 
     file = here::here("data", "LOTT_simulations.rda"))
```

```{r, echo = FALSE}
load(here::here("data", "LOTT_simulations.rda"))
```


Now we will combine the output so that we can make a plot to visualize the results that we obtained. First let's name each subset that we created.

```{r}
names(subsets_models_DONOHUE) <-     paste0("DONOHUE_",1:length(subsets_models_DONOHUE))

names(subsets_models_LOTT) <- 
paste0("LOTT_",1:length(subsets_models_LOTT))

```

Now we can combine the tibbles within the list of tibbles for the `subsets_models_DONOHUE` and `subsets_models_LOTT` data.

To do this we will use the `bind_rows()` function of the `dplyr` package with the `.id = "ID"` argument, which will create a new variable called `ID` that will list the name of the tibble the data came from.

Then we will combine the data from both the Donohue and Lott simulations.

```{r}

simulations_DONOHUE <- subsets_models_DONOHUE %>%
  bind_rows(.id = "ID") %>%
  mutate(Analysis = "Analysis 1")

simulations_LOTT <- subsets_models_LOTT %>%
  bind_rows(.id = "ID") %>%
  mutate(Analysis = "Analysis 2")

simulations <- bind_rows(simulations_DONOHUE,
                         simulations_LOTT)

head(simulations)
tail(simulations)
```

Now we will make a jitter plot using the `geom_jitter()` function of the coefficient estimates of the `RTC_LAWTRUE` variable for each simulation.

Since there are many variables in both analyses, we will use the `facet_grid()` function of the `ggplot2` package to allow us to separate the data for each analysis into subplots. The argument `scale = "free_x"` and `drop = TRUE` allow us to only include the variables that were present in Analysis 1, as opposed to empty spots for the variables that were in Analysis 2 but not in Analysis 1. The `space = "free"` argument removes the extra space from the dropped variables. 

#### {.question_block}
<b><u> Question Opportunity </u></b>

What happens if don't use the `drop = TRUE` argument or the `space = "free"` argument? 

####

```{r}
simulation_plot <- simulations %>%
  ggplot(aes(x = term, y = estimate)) + 
  geom_jitter(alpha = 0.25,
              width = 0.1) + 
  facet_grid(.~Analysis, scale = "free_x", space = "free", drop = TRUE) +
  labs(title = "Coefficient instability",
       subtitle = "Estimates sensitive to observation deletions",
       x = "Term",
       y = "Coefficient",
       caption = "Results from simulations") + 
  theme_linedraw() +
  theme(axis.title.x = element_blank(),
        axis.text.x = element_text(angle = 70, hjust = 1),
        strip.text.x = element_text(size = 14, face = "bold"))

simulation_plot

```

We can see that the range of coefficient estimates when only one observation is removed is much larger in Analysis 2 for nearly all variables, but particularly for many of the additional demographic variables.

Let's make a plot showing the summary of the overall coefficient instability.

To do this we will calculate the standard deviation of coefficient estimates for each variable across all of the simulations. Thus  we will group  by  the `Analysis` and  the `term` variables now that our data is in long format. We will use the `sd()` function of the `stats` package to calculate the standard deviation.

```{r}
coeff_sd <-simulations %>% 
  group_by(Analysis, term) %>% 
  summarise("SD" = sd(estimate))
```

```{r}
library(DT)
DT::datatable(coeff_sd)
```

Now we will make a plot of this data.

```{r}
simulation_plot <- coeff_sd %>%
  ggplot(aes(x = Analysis, y = SD)) + 
 geom_jitter(width = 0.1, alpha = 0.5, size = 2) + 
  labs(title = "Coefficient instability",
       subtitle = "Coefficient estimates are unstable",
       x = "Term",
       y = "Coefficient Estimate \n Standard Deviations",
       caption = "Results from simulations") + 
  theme_minimal() +
  theme(axis.title.x = element_blank(),
         axis.text.x = element_text(size = 8, color = "black"),
         axis.text.y = element_text(color = "black"))
simulation_plot

```

Here we can clearly see that overall the coefficient estimates are much less stable in Analysis 2. This is a clear indication that we have multicollinearity.

from Michael
```{r, eval = FALSE}
sims <- 250

# DONOHUE
samps_DONOHUE <- lapply(rep(dim(DONOHUE_DF)[1]/2, sims),
       function(x)DONOHUE_DF[sample(nrow(DONOHUE_DF),
                                     size = x, replace = FALSE),])

fit_nls_on_bootstrap_DONOHUE <- function(split){
  plm(Viol_crime_rate_1k_log ~
                        RTC_LAW +
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log + 
                        police_per_100k_lag,
      data = data.frame(split),
      index = c("STATE","YEAR"),
      model = "within",
      effect = "twoways")
}
  
samps_models_DONOHUE <- lapply(samps_DONOHUE, fit_nls_on_bootstrap_DONOHUE)

samps_models_DONOHUE <- samps_models_DONOHUE %>%
  map(tidy)

names(samps_models_DONOHUE) <- paste0("DONOHUE_",1:length(samps_models_DONOHUE))

simulations_DONOHUE <- samps_models_DONOHUE %>%
  bind_rows(.id = "ID") %>%
  mutate(Analysis = "Analysis 1")

## LOTT

samps_LOTT <- lapply(rep(round(dim(LOTT_DF)[1]/2), sims),
       function(x) LOTT_DF[sample(nrow(LOTT_DF),
                                  size = x, replace = FALSE),])

fit_nls_on_bootstrap_LOTT <- function(split){
  plm(LOTT_fmla,
      data = data.frame(split),
      index = c("STATE","YEAR"),
      model = "within",
      effect = "twoways")
}
  
samps_models_LOTT <- lapply(samps_LOTT, fit_nls_on_bootstrap_LOTT)

samps_models_LOTT <- samps_models_LOTT %>%
  map(tidy)

names(samps_models_LOTT) <- paste0("LOTT_",1:length(samps_models_LOTT))

simulations_LOTT <- samps_models_LOTT %>%
  bind_rows(.id = "Analysis") %>%
  mutate(Analysis = "Analysis 2")

simulations <- bind_rows(simulations_DONOHUE,
                         simulations_LOTT)

simulation_plot <- simulations %>%
  filter(term=="RTC_LAWTRUE") %>%
  ggplot(aes(x = Analysis, y = estimate)) + 
  geom_boxplot(alpha = 0.25,
              width = 0.1) + 
  labs(title = "Coefficient instability",
       subtitle = "Estimates sensitive to observation deletions",
       x = "Term",
       y = "Coefficient",
       caption = "Results from simulations") + 
  theme_minimal() +
  theme(axis.title.x = element_blank())

simulation_plot
```




### VIF

Another way of evaluating the presence and severity of multicollinearity is to calculate the [variance inflation factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor){target="_blank"} . 

According to [Wikipedia](https://en.wikipedia.org/wiki/Variance_inflation_factor){target="_blank"}:

>It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity. 

> The variance inflation factor (VIF) is the **quotient of the variance in a model with multiple terms by the variance of a model with one term alone**.

VIF values can be calculated for each explanatory variable in a model by performing the following calculation:

1) Run another ordinary least squares (OLS) linear regression with one of the explanatory  variables of your model of interest ($Xi$) as the dependent variable and keep the remaining explanatory variables as explanatory variables.

So generically speaking say this is our model:

$$Y = β_0 + β_1X_1 + β_2X_2 + β_3X_3 $$
We have three explanatory variables ($X_1$, $X_2$, and $X_3$).

If we want to calculate the VIF value for $X_1$ we would need to perform another OLS model, where $X1$ is now the dependent variable explained by the other explanatory variables.

$$X_1 = β_0 +  β_2X_2 + β_3X_3 $$

The [$R^2$ coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination){target="_blank"} also called R squared value from this regression is then used to calculate the VIF as follows:

$$\frac{1}{1-R^{2}}$$

The $R^2$ value is in this case the proportion of variance in $X_1$ explained by the other variables ($X_2$ and $X_3$).


VIF values are typically calculated for each explanatory variable when evaluating multicollinearity of a model.

Thus overall the calculation is:
$$VIF_i = \frac{1}{1-R_i^{2}}$$
Where $i$ corresponds to each explanatory variable.


Recall that according to [Wikipedia](https://en.wikipedia.org/wiki/Variance_inflation_factor){target="_blank"}:

> The variance inflation factor (VIF) is the **quotient of the variance in a model with multiple terms by the variance of a model with one term alone**.

The $R^2$ value ranges from 0 to 1. Thus, if a variation of a variable is highly explained by the other variables, the $R^2$ will approach 1. Thus the denominator in the VIF calculation $1-R_i^{2}$ (which is sometimes referred to as tolerance) will be smaller and the VIF value will be larger.

Thus, **higher VIF vales** indicate **more severe multicollinearity**. Typically a threshold of a tolerance of less than 0.10 and/or a VIF of 10 and above is used as a rule of thumb to determine if the presence of multicollinearity might be problematic.

Please see this [article](https://link.springer.com/content/pdf/10.1007/s11135-006-9018-6.pdf){target="_blank"} for a thorough explanation of how to interpret VIF values and how to decide what to do if your model has high multicollinearity.

So how do we calculate VIF values in R?

We could do this manually creating many linear regressions, but that would obviously be time consuming. Luckily, the `car` package has a function called `vif()` that will calculate VIF values.


avocado: Idk if this is true or if we need to do what Michael did?
https://stackoverflow.com/questions/20281055/test-for-multicollinearity-in-panel-data-r

Since VIF values are calculated using only the independent variables, we don't need to worry about the effect of time and individuals in our calculation, thus we will use the pooled model. Otherwise, the `vif()` function is incompatible with the output from the `plm()` function.

Then we will create nicer looking output of the data using the `as_tibble()`function of the `tibble` package to create a tibble and add the variable names as another column.

```{r}

vif_DONOHUE <- vif(DONOHUE_pool_model)
vif_DONOHUE 

vif_DONOHUE <- vif_DONOHUE %>%
  as_tibble() %>%
  cbind(., names(vif_DONOHUE))
  
colnames(vif_DONOHUE) <- c("VIF", "Variable")

vif_DONOHUE
```

Now we will do the same for the Lott data:

```{r}
LOTT_pool_model <-plm(LOTT_fmla,
                        model = "pooling",
                        data=d_panel_LOTT)

vif_LOTT <- vif(LOTT_pool_model)


vif_LOTT <- vif_LOTT %>%
  as_tibble() %>%
  cbind(., names(vif_LOTT))
colnames(vif_LOTT) <- c("VIF", "Variable")
```


```{r}

DT::datatable(vif_LOTT)
```

We can see that some of the VIF values are very high!



avocado:From Michael: he calculated the time-demeaned response variable based on this link:
http://karthur.org/2019/implementing-fixed-effects-panel-models-in-r.html

Idk if we need to/should do this. The values are different.- lower in general.If so though this is how one would do it.

This `vif()` function is not compatible with the `plm` package panel data frames and linear model outputs, thus we need to create a design matrix with the `plm` package using the `Within()` function and then use the `lm()` function of the `stats` package to fit the model.

```{r, eval = FALSE}
design.matrix <- as.data.frame(model.matrix(DONOHUE_OUTPUT))
#design.matrix<-DONOHUE_OUTPUT[["model"]]
design.matrix$Viol_crime_rate_1k_log <- plm::Within(
  d_panel_DONOHUE$Viol_crime_rate_1k_log)

lm_DONOHUE <- lm(Viol_crime_rate_1k_log ~
                        RTC_LAWTRUE + # logical class changes variable name after inital model
                        White_Male_15_to_19_years +
                        White_Male_20_to_39_years +
                        Black_Male_15_to_19_years +
                        Black_Male_20_to_39_years +
                        Other_Male_15_to_19_years +
                        Other_Male_20_to_39_years +
                        Unemployment_rate +
                        Poverty_rate + 
                        Population_log +
               police_per_100k_lag,
             data = design.matrix)


vif_DONOHUE <- vif(lm_DONOHUE)

vif_DONOHUE

vif_DONOHUE <- vif_DONOHUE %>%
  as_tibble() %>%
  cbind(., names(vif_DONOHUE)) %>%
  as_tibble()
  
colnames(vif_DONOHUE) <- c("VIF", "Variable")
```

```{r, eval = FALSE}
design.matrix <- as.data.frame(model.matrix(LOTT_OUTPUT))

design.matrix$Viol_crime_rate_1k_log <- plm::Within(
  d_panel_LOTT$Viol_crime_rate_1k_log)

LOTT_variables_ols <- LOTT_DF %>%
  dplyr::select(RTC_LAW,
                contains(c("White","Black","Other")),
                Unemployment_rate,
                Poverty_rate,
                Population_log,
                police_per_100k_lag) %>%
  colnames() %>%
  str_replace("RTC_LAW", "RTC_LAWTRUE") # logical class changes variable name after inital model

LOTT_fmla_ols <- as.formula(paste("Viol_crime_rate_1k_log ~",
                              paste(LOTT_variables_ols, collapse = " + ")
                              )
                        )

lm_LOTT <- lm(LOTT_fmla_ols,
             data = design.matrix)

vif(lm_LOTT)

vif_LOTT <- vif(lm_LOTT)

vif_LOTT <- vif_LOTT %>%
  as_tibble() %>%
  cbind(., names(vif_LOTT)) %>%
  as_tibble()
  
colnames(vif_LOTT) <- c("VIF", "Variable")

max_vif_LOTT <- max(vif(lm_LOTT))
```


Now we will make a plot of the VIF values for both analyses:

```{r}

vif_DONOHUE$Analysis <- "Analysis 1"
vif_LOTT$Analysis <- "Analysis 2"

vif_df <- rbind(vif_DONOHUE,
                vif_LOTT)

vif_plot <- vif_df %>%
  ggplot(aes(x = Analysis, y = VIF)) +
  geom_jitter(width = 0.1, alpha = 0.5, size = 2) +
  geom_hline(yintercept = 10, color = "red") +
  scale_y_continuous(trans = 'log10',
                     limits = c(1,1000)) +
  labs(title = "Variance inflation factors") + 
  theme_minimal() +
  theme(axis.title.x = element_blank(),
         axis.text.x = element_text(color = "black"),
         axis.text.y = element_text(color = "black"))

vif_plot
```
We can see that both analyses have some variables with relatively high multicollinearity, however analysis 2 has variables with very high multicollinearity.

In many cases it would be advisable to remove one or more of these variables and reassess the VIF values. There are also other options, such as [ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization){target="_blank"}. However, both of these options need to be done with care as they can also introduce bias into the model.

In any case the presence of multicollinearity should encourage further investigation about the design of the model, as the results may not be reliable due to the increased level of instability of the coefficients estimates.

See this this [article](https://link.springer.com/content/pdf/10.1007/s11135-006-9018-6.pdf){target="_blank"} for a detailed discussion about what to consider when your model has variables with high VIF values. 


# **Data Visualization**
*** 

Now lets make a plot that summarizes all of our findings.

We will use the `cowplot` package to put our plots together. 

We will use the `ggdraw()` function of this package. This allows you to add labels and other plot aspects on top of existing plots. Thus if we want to add a title element to our overall plot that we will add to a combined plot of our existing plots we can use `ggdraw()` to start and then the `draw_label()` function to add text.


```{r, fig.height=10, echo=FALSE, message=FALSE, warning=FALSE}
title_plots <- ggdraw() + 
  draw_label(
    "Multicollinearity and its effects",
    fontface = 'bold',
    size=18,
    x = 0,
    hjust = -0.01
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

title_plots
class(title_plots)
```

As you can see we know have plot object that just has the text `"Multicollinearity and its effects"`.

Now we will create a subtitle in the same way.

```{r}

forward <- ggdraw() + 
  draw_label(
    "Analysis 1: 6 demographic variables\nAnalysis 2: 36 demographic variables",
    fontface = 'bold',
    size=10,
    x = 0,
    hjust = -0.02
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

forward
```

Now we will recreate our correlation plots with some slight alterations. We want to remove our labels, because they will be too small to see when we combine our plots. To do this we will use the `theme_void()` function of the `ggplot2` package.

Note that because we are layering ggplot2 objects we can't use the `%>%` pipe to start with the existing correlation plots.

```{r}

corr_mat_DONOHUE <- corr_mat_DONOHUE +
  theme_void() + 
  theme(plot.title= element_text(size = 8, color = "black")) +
  labs(title = "Analysis 1") 

corr_mat_LOTT <- corr_mat_LOTT +
  theme_void() + 
  theme(plot.title= element_text(size = 8,color = "black")) +
  labs(title = "Analysis 2") 
```

OK we want to arrange our correlation plots to be in the top row of our larger plot. Now we will use the `plot_grid()` function to arrange the plots.

```{r}
row_A <- plot_grid(corr_mat_DONOHUE,
                   corr_mat_LOTT,
                   nrow = 1)
row_A
```


Nice! We have combined plots!


Now let's add a title for these plots.

```{r}

title_A <- ggdraw() + 
  draw_label(
    "Correlation between variables can induce multicollinearity",
    fontface = 'bold',
    size=14,
    x = 0,
    hjust = -0.01
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

plot_A <- plot_grid(title_A,
                    row_A,
                    ncol = 1,
                    rel_heights = c(0.1,1))
```


For our second row in our larger plot we want to have the formula for calculating VIF values on the left and the plot that we created previously showing VIF values on the right.

First we will create a plot object that just has the formula.

To do so we are going to create a plot with a large label in the middle containing the formula. Then we will use the `theme_void()` function again to remove the axis labels and background.

To create our plot we will first plot values from 1-10 for both the x and y axis, allowing us to center the formula at x and y values of 5.

To type the formal we need to use [LaTeX mathematical notation](https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html){target="_blank"}.

The start and end of inline mathematical formulas are specified using dollar signs (`$`).   
Subscripts are written by using an underscore (`_`) and brackets (`{}`) indicate the start and end of the subscript.   

Fractions are indicated using `\frac{nominator}{denominator}`.

Superscripts are created using the carrot symbol (`^`) and brackets (`{}`) indicate the start and end of the superscript.   

Greek letters  are created by using `\beta`

In the case of the fraction and Greek letters an additional `\` is needed in the `Tex()` function.

We will use the `TeX()` function of the `latex2exp` package to convert our LaTeX string to a [plotmath expression](https://stat.ethz.ch/R-manual/R-devel/library/grDevices/html/plotmath.html){target="_blank"} (a mathematical notation in R to be used in plots.)

```{r}
empty_df <- cbind(c(1:10),c(1:10)) %>%
  as.data.frame()

colnames(empty_df) <- c("X", "Y")

plot_B1 <- ggplot(empty_df, aes(x = X, y = Y)) +
   annotate("text",
           x=5,
           y=8,
           label = TeX("$X_{1} = \\beta_{0} + \\beta_{2}X_{2} + \\beta_{3}X_{3}...+\\beta_{k}X_{k}$"),
           size = 7)+ ylim(0,10)+xlim(0,10)+
   annotate("text",
           x=5.9,
           y=5.5,
           label = TeX("$R^{2}$"),
           size = 7)+ ylim(0,10)+xlim(0,10)+
geom_segment(aes(x = 5, y = 6, xend = 5, yend = 4.5),
               arrow = arrow(angle = 45, ends = "last", type = "open"),
               size = 1.8,
               color = "black",
               lineend = "butt",
               linejoin = "mitre") +
  annotate("text",
           x=5,
           y=2,
           label = TeX("$VIF_{i} = \\frac{1}{1-R_{i}^{2}}$"),
           size = 7)

  
 
  
plot_B1

plot_B1 <-plot_B1+
  theme_void()

plot_B1
```

Now we will combine this with the VIF plot.

```{r}
plot_B2 <- vif_plot +
  theme(axis.text.x = element_text(size=8))

row_B <- plot_grid(plot_B1,
                   plot_B2,
                   nrow = 1)

title_B <- ggdraw() + 
  draw_label(
    "Variance inflation factors can be used to identify multicollinearity when present",
    fontface = 'bold',
    size = 14,
    x = 0,
    hjust = -.01,
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

plot_B <- plot_grid(title_B,
                    row_B,
                    ncol = 1,
                    rel_heights = c(0.1,1))

plot_B
```


Now for the third row we want to include the `comparing_analyses_plot` and the `simulation_plot`.

```{r}
plot_C1 <- comparing_analyses_plot + 
  theme(axis.text.x = element_text(size = 8),
        axis.title.x = element_blank()) +
  labs(title = "Introduces bias to estimates",
       subtitle = "Bias introduced can change direction of estimate")

plot_C2 <- simulation_plot +
  labs(title = "Reduces precision in estimates")

row_C <- plot_grid(plot_C1,
                   plot_C2,
                   nrow = 1)

title_C <- ggdraw() + 
  draw_label(
    "Multicollinearity can have a negative effect on statistical inference",
    fontface = 'bold',
    size=14,
    x = 0,
    hjust = -0.01
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

plot_C <- plot_grid(title_C,
                    row_C,
                    ncol = 1,
                    rel_heights = c(0.1,1))

plot_C
```


Now that we have all of our rows we can combine everything together.

```{r}
plots <- plot_grid(plot_A,
                   plot_B,
                   plot_C,
          ncol = 1,
          rel_heights = c(1,1,1))

mainplot <- plot_grid(title_plots,
                       forward,
                       plots,
                       ncol = 1,
                       rel_heights = c(0.05,
                                       0.05,
                                       1))

mainplot


```

from Michael
```{r, eval = FALSE}
plot_A1 <- corr_mat_DONOHUE

plot_A2 <- corr_mat_LOTT

row_A <- plot_grid(plot_A1,
                   plot_A2,
                   nrow = 1)

title_A <- ggdraw() + 
  draw_label(
    "Correlation between variables can induce multicollinearity",
    fontface = 'bold',
    size=14,
    x = 0,
    hjust = -.01
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

legend_A <- get_legend(corr_mat_LOTT)

plot_A <- plot_grid(title_A,
                    row_A,
                    ncol = 1,
                    rel_heights = c(0.1,1))

empty_df <- cbind(c(1:10),c(1:10)) %>%
  as.data.frame()

colnames(empty_df) <- c("X", "Y")

plot_B1 <- ggplot(empty_df, aes(x = X, y = Y)) +
  annotate("text",
           x=5,
           y=5,
           label = TeX("$VIF_{i} = \\frac{1}{1-R_{i}^{2}}$"),
           size = 8) +
  theme_void()

plot_B2 <- vif_plot +
  theme(axis.title.x = element_text(size=8))

row_B <- plot_grid(plot_B1,
                       plot_B2,
                       nrow = 1)

title_B <- ggdraw() + 
  draw_label(
    "Variance inflation factors can be used to identify multicollinearity when present",
    fontface = 'bold',
    size=14,
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

plot_B <- plot_grid(title_B,
                    row_B,
                    ncol = 1,
                    rel_heights = c(0.1,1))

plot_C1 <- comparing_analyses_plot + 
  theme(axis.text.x = element_text(size = 8),
        axis.title.x = element_blank()) +
  labs(title = "Introduces bias to estimates",
       subtitle = "Bias introduced can change direction of estimate")

plot_C2 <- simulation_plot +
  labs(title = "Reduces precision in estimates")

row_C <- plot_grid(plot_C1,
                       plot_C2,
                       nrow = 1)

title_C <- ggdraw() + 
  draw_label(
    "Multicollinearity can have a negative effect on statistical inference",
    fontface = 'bold',
    size=14,
    x = 0,
    hjust = 0
  ) +
  theme(
    plot.margin = margin(0, 0, 0, 0)
  )

plot_C <- plot_grid(title_C,
                    row_C,
                    ncol = 1,
                    rel_heights = c(0.1,1))

plots <- plot_grid(plot_A,
                   plot_B,
                   plot_C,
          ncol = 1,
          rel_heights = c(1,1,1))

mainplot <- plot_grid(title_plots,
                       forward,
                       plots,
                       #legend_uw,
                       ncol = 1,
                       rel_heights = c(0.05,
                                       0.05,
                                       1))

mainplot
```



```{r, echo=FALSE, include=FALSE}
ggsave(here::here("img", "mainplot.png"))
```




# **Summary**
*** 

This case study has introduced the concept of multicollinearity by exploring data related to violent crimes and right-to-carry gun laws. We also introduced the topic of panel data as a special type of longitudinal data that includes data of 2 or more individuals or groups over 2 or more time points. We learned that we can use the `plm` package to perform panel linear regression analysis. We learned that the fixed effect model in panel analysis actually makes the least assumptions, and is therefore often the most appropriate test.

By evaluating two analyses that were identical except for the inclusion of extra demographic variables, analysis 1 included 6, while analysis 2 included 36, we discovered that redundant and collinear variables can change the directionality and magnitude of our findings. 

We learned that by looking at the correlation between pairs of explanatory variables we can get a sense about whether multicollinearity may exist in our data.

We learned that we can evaluate the stability of our coefficient estimates across sub-samples or calculate variance inflation factor (VIF) values to get a sense of the presence and severity of multicollinearity.

We learned that often a rule of thumb of >10 is used as a threshold for raising concern about the severity of multicollinearity. However, we also learned that (as often is the case with thresholds) more care may be required. 

Overall we learned that multicollinearity can bias our regression findings and it is good practice to check for multicollinearity when performing regression analysis.  It is something to keep in mind when we encounter coefficient estimates that are unexpected. 

Importantly this case study showcases how methodological details, like how we decide to parse our demographic variables can have great consequences on the results of our analyses. 


# **Suggested Homework**
*** 

Ask students to remove one or more of the demographic variables with high VIF values from the Lott-like panel data and perform the panel linear regression analysis again, as well as calculate the VIF values. 

Ask the students to discuss how this possibly changed the results.


# **Additional Information**
***

## Helpful Links

[Tidyverse](https://www.tidyverse.org/){target="_blank"}  
[Writing functions](https://r4ds.had.co.nz/functions.html){target="_blank"}   
Also see [this case study](https://opencasestudies.github.io/ocs-bloomberg-vaping-case-study/){target="_blank"} for more information on writing functions      
Please see [this case study](https://opencasestudies.github.io/ocs-bp-co2-emissions/){target="_blank"}  for more details on using `ggplot2`     
[Longitudinal studies](https://www.bmj.com/about-bmj/resources-readers/publications/epidemiology-uninitiated/7-longitudinal-studies){target="_blank"}   
[Panel data](https://en.wikipedia.org/wiki/Panel_data){target="_blank"}    
[Confidence intervals](https://en.wikipedia.org/wiki/Confidence_interval){target="_blank"}   
[Linear regression](https://en.wikipedia.org/wiki/Linear_regression){target="_blank"}  
[panel regression analysis](https://en.wikipedia.org/wiki/Panel_analysis){target="_blank"}   
[Hausmen test](https://en.wikipedia.org/wiki/Durbin%E2%80%93Wu%E2%80%93Hausman_test){target="_blank"} 
[Resampling](https://en.wikipedia.org/wiki/Resampling_(statistics)){target="_blank"}   
[Variance inflation factor (VIF)](https://en.wikipedia.org/wiki/Variance_inflation_factor){target="_blank"}   
[$R^2$ coefficient of determination](https://en.wikipedia.org/wiki/Coefficient_of_determination){target="_blank"}   
[Ridge regression](https://en.wikipedia.org/wiki/Tikhonov_regularization){target="_blank"}  
[LaTeX mathematical notation](https://www.calvin.edu/~rpruim/courses/s341/S17/from-class/MathinRmd.html)target="_blank"}   

For more information on linear regression see this [book](https://rafalab.github.io/dsbook/linear-models.html#linear-regression-in-the-tidyverse){target="_blank"} and this [case study](https://opencasestudies.github.io/ocs-bp-diet/){target="_blank"}.

For more information on the different types of panel regression models see this [book](https://bookdown.org/ccolonescu/RPoE4/panel-data-models.html),  [here](https://www.bauer.uh.edu/rsusmel/phd/ec1-15.pdf), and [here](https://sites.google.com/site/econometricsacademy/econometrics-models/panel-data-models).

For more information on implementing panel regression in R using the `plm` package, see [here](https://cran.r-project.org/web/packages/plm/vignettes/plmPackage.html){target="_blank"}  and [here](http://www.princeton.edu/~otorres/Panel101R.pdf){target="_blank"}.

For more information on multicollinearity and VIF, see this [article](https://link.springer.com/content/pdf/10.1007/s11135-006-9018-6.pdf){target="_blank"}.DOI 10.1007/s11135-006-9018-6

The articles used to motivate this case study are:   
[Lott and Mustard](https://chicagounbound.uchicago.edu/cgi/viewcontent.cgi?article=1150&context=law_and_economics){target="_blank"}  
[Donohue, et al.](https://www.nber.org/papers/w23510.pdf){target="_blank"}     
[See here for a list of studies on this topic ](https://en.wikipedia.org/wiki/More_Guns,_Less_Crime){target="_blank"}  

avocadoPossible tutorials about plm: (not sure if we want to include or not)  
https://rpubs.com/rslbliss/fixed_effects  
http://karthur.org/2019/implementing-fixed-effects-panel-models-in-r.html  


avocado possible discussion about if and when high vif values are a problem:
https://statisticalhorizons.com/multicollinearity

## Session Info

```{r}
sessionInfo()
```

## Acknowledgements

We would like to acknowledge [Daniel Webster](https://www.jhsph.edu/faculty/directory/profile/739/daniel-webster) for assisting in framing the major direction of the case study.

We would also like to acknowledge the [Bloomberg American Health Initiative](https://americanhealth.jhu.edu/) for funding this work. 

